{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"RSG_Exercises_Tomato_Intermediate.ipynb","provenance":[],"collapsed_sections":["EW_RbYrrRXOo","Wp2Ej74byTL-","c4lR3qZZluTM","wgw_WVucp1l5","FqoG0D16QxhL","h14kFcdUVF8X","n4HyqB-BW4xQ","iA3K-MgHdvIk","-R4Lbl8GfG4O","KCZ7IVa5h2a1"],"authorship_tag":"ABX9TyOIbpq1NVa1/8aPKOIIx/Rh"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"1diqC4dBz1xc"},"source":["# Ready, Steady, Go AI (*Exercises*)"]},{"cell_type":"markdown","metadata":{"id":"MlQSL0no0uYT"},"source":["This notebook is a supplement to the paper, **Ready, Steady, Go AI: A Practical Tutorial on Explainable Artificial Intelligence and Its Applications in Phenomics Image Analysis** (submitted to *Patterns, 2021*) by Farid Nakhle and Antoine Harfouche\n","\n","Read the accompanying paper [here](https://doi.org)."]},{"cell_type":"markdown","metadata":{"id":"X5XoL1aD07Qo"},"source":["# Table of Contents\n"]},{"cell_type":"markdown","metadata":{"id":"SV7BfDp03T2j"},"source":["* **1. Introduction**\n","* **2. Exercise I: Splitting Data**\n","* **3. Exercise II: Cropping Leaf Images**\n","* **4. Exercise III: Segmenting Leaf Images**\n","* **5. Exercise IV: Descriptive Data Analysis**\n","* **6. Exercise V: Balancing the Dataset**\n","* **7. Exercise VI: Classification Using DenseNet-161 Pretrained DCNN algorithm**\n","* **8. Exercise VII: Generating Confusion Matrix**\n","* **9. Exercise VIII: Generating Explanations With LIME**"]},{"cell_type":"markdown","metadata":{"id":"7btTy9923b9k"},"source":["# 1. Introduction\n"]},{"cell_type":"markdown","metadata":{"id":"iVEjYGUW3iMI"},"source":["Before attempting to resolve the exercises found in this notebook, visit our Github repository and try to open and run all the notebooks provided by the tutorial. \n","\n","Here, the solution for each exercise can be found in a hidden code cell at its end.\n","\n","Users should try to solve the exercises with the help of the notebooks provided by the tutorial before looking at the solution."]},{"cell_type":"markdown","metadata":{"id":"OnMTvK_SBSro"},"source":["As a reminder, we are working with the PlantVillage dataset, originally obtained from [here](http://dx.doi.org/10.17632/tywbtsjrjv.1).\n","For the following exercises, we will be working with a subset of PlantVillage containing the tomato classes only. We have made the subset available [here](http://faridnakhle.com/pv/tomato-original.zip). \n","\n","**It is important to note that Colab deletes all unsaved data once the instance is recycled. Therefore, remember to download your results once you run the code.**"]},{"cell_type":"markdown","metadata":{"id":"5n-qgOHeBPuO"},"source":["#2. Exercise I: Data Splitting\n"]},{"cell_type":"markdown","metadata":{"id":"hmP2F-VcR8RX"},"source":["**A.** Complete the code that downloads the PlantVillage tomato leaves dataset using the link provided in the introduction. The dataset must be saved then extracted to /content/dataset/original/.\n","\n","**B.** Complete the code that randomly splits the dataset into training, validation, and testing. Use the following split ratio: training: 80%, validation: 10%, testing: 10%. The split dataset must be saved under /content/dataset/split/"]},{"cell_type":"code","metadata":{"id":"LQid3v_LSdi8"},"source":["import requests\n","import os\n","import zipfile\n","\n","## Write a code using http requests to download the dataset zip file.\n","## Use zipfile software library to extract the dataset to /content/dataset/original\n","\n","\n","# USE PIP to install Splitfolders\n","\n","# Replace \"?\" with your answer. NB: splitfolders expect the ratio parameter in the following format: for 50%, type .5; for 60%, .6; etc.\n","!splitfolders --output ? --seed 1337 --ratio ? ? ?  -- \"/content/dataset/original\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EW_RbYrrRXOo"},"source":["# Solution"]},{"cell_type":"code","metadata":{"id":"2QkBhoIt66D-"},"source":["!rm -R /content/dataset/original/\n","!rm -R /content/dataset/split/\n","\n","import requests\n","import os\n","import zipfile\n","\n","dataset_url = \"http://faridnakhle.com/pv/tomato-original.zip\"\n","save_data_to = \"/content/dataset/original/\"\n","dataset_file_name = \"dataset.zip\"\n","\n","if not os.path.exists(save_data_to):\n","    os.makedirs(save_data_to)\n","\n","r = requests.get(dataset_url, stream = True, headers={\"User-Agent\": \"Ready, Steady, Go AI\"})\n","\n","print(\"Downloading dataset...\")  \n","\n","with open(save_data_to + dataset_file_name, \"wb\") as file: \n","    for block in r.iter_content(chunk_size = 1024):\n","         if block: \n","             file.write(block)\n","\n","## Extract downloaded zip dataset file\n","print(\"Dataset downloaded\")  \n","print(\"Extracting files...\")  \n","with zipfile.ZipFile(save_data_to + dataset_file_name, 'r') as zip_dataset:\n","    zip_dataset.extractall(save_data_to)\n","\n","## Delete the zip file as we no longer need it\n","os.remove(save_data_to + dataset_file_name)\n","print(\"All done!\")  \n","\n","## SPLIT \n","!pip install split-folders tqdm\n","!splitfolders --output \"/content/dataset/split/\" --seed 1337 --ratio .8 .1 .1 -- \"/content/dataset/original\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r67fWPR_yLKl"},"source":["# 3. Exercise II: Cropping Leaf Images\n","\n","Before you start, make sure to run the \"Install and import prequisites\", \"Download pretrained model\", and \"Define prerequisite functions\" code cells.\n","Once you run all of them, you are required to pass the path of the folder containing the split data from Exercise I (training, validation, and testing) to the crop function below. Running it will trigger YOLO to crop the images using a pretrained model. \n","After that, you should be able to run the \"Preview a Cropped Image\" cell and see a sample image from the results."]},{"cell_type":"code","metadata":{"cellView":"form","id":"isSivclMykq1"},"source":["#@title Install and import prerequisites\n","!git clone https://github.com/ultralytics/yolov3\n","%cd yolov3\n","%pip install -qr requirements.txt \n","import torch\n","from IPython.display import Image\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"GBK4Ma_ZyM1c"},"source":["#@title Download pretrained model\n","\n","model_URL = \"http://faridnakhle.com/pv/models/YOLOv3.zip\"\n","save_data_to = \"/content/models/\"\n","model_file_name = \"yolo.zip\"\n","\n","if not os.path.exists(save_data_to):\n","    os.makedirs(save_data_to)\n","\n","print(\"Downloading model...\")  \n","\n","r = requests.get(model_URL, stream = True, headers={\"User-Agent\": \"Ready, Steady, Go AI\"})\n","with open(save_data_to + model_file_name, \"wb\") as file: \n","    for block in r.iter_content(chunk_size = 1024):\n","         if block: \n","             file.write(block)\n","\n","## Extract downloaded zip dataset file\n","print(\"Model downloaded\")  \n","print(\"Extracting files...\")\n","\n","with zipfile.ZipFile(save_data_to + model_file_name, 'r') as zip_dataset:\n","    zip_dataset.extractall(save_data_to)\n","print(\"All done!\")  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"QLqPXyPq4dG0"},"source":["#@title Define prerequisite functions\n","import os\n","import cv2\n","import random\n","import numpy as np\n","\n","# function for cropping each detection and saving as new image\n","def crop_objects(img, data, path):\n","    boxes, scores, classes, num_objects = data\n","    #create dictionary to hold count of objects for image name\n","    for i in range(len(num_objects)):\n","        # get count of class for part of image name\n","        class_index = int(classes[i])\n","        # get box coords\n","        xmin, ymin, xmax, ymax = boxes[i]\n","        # crop detection from image\n","        cropped_img = img[int(ymin)-5:int(ymax)+5, int(xmin)-5:int(xmax)+5]\n","        # construct image name and join it to path for saving crop properly\n","        img_name =  'cropped_img.png'\n","        img_path = os.path.join(path, img_name )\n","        # save image\n","        cv2.imwrite(img_path, cropped_img)\n","\n","def crop_object(img, coords, img_path):\n","    # get box coords\n","    xmin = int(coords[0])\n","    ymin = int(coords[1])\n","    xmax = int(coords[2])\n","    ymax = int(coords[3])\n","    # crop detection from image\n","    cropped_img = img[ymin:ymax, xmin:xmax]\n","    # save image\n","    cv2.imwrite(img_path, cropped_img)\n","\n","def plot_grid(img, line_color=(0, 255, 0), thickness=1, type_=cv2.LINE_AA, pxstep=20, pystep=20):\n","    x = pystep\n","    y = pxstep\n","\n","    while x < img.shape[1]:\n","        cv2.line(img, (x, 0), (x, img.shape[0]), color=line_color, lineType=type_, thickness=thickness)\n","        x += pystep\n","\n","    while y < img.shape[0]:\n","        cv2.line(img, (0, y), (img.shape[1], y), color=line_color, lineType=type_, thickness=thickness)\n","        y += pxstep\n","def plot_borders(img, line_color=(0, 255, 0), thickness=1):\n","    cv2.rectangle(img,(0 ,0),(img.shape[1]-thickness,img.shape[0]-thickness), line_color, thickness)\n","\n","def myround(x, base=5):\n","    return base * round(x/base)\n","def plot_overlay(x, img, color, alpha,\n"," pxstep=20, pystep=20):\n","    overlay = img.copy()\n","    x0, x1, x2, x3 = int(x[0]), int(x[1]), int(x[2]), int(x[3])\n","\n","    x0 = myround(x0,pystep)\n","    x1 = myround(x1,pxstep)\n","    x2 = myround(x2,pystep)\n","    x3 = myround(x3,pxstep)\n","\n","    c1, c2 = (x0, x1), (x2, x3)\n","    cv2.rectangle(overlay, c1, c2, color, -1)\n","    # apply the overlay\n","    cv2.addWeighted(overlay, alpha, img, 1 - alpha, 0, img)\n","\n","!cd /content/yolov3/\n","import argparse\n","import time\n","from pathlib import Path\n","\n","import cv2\n","import torch\n","import torch.backends.cudnn as cudnn\n","from numpy import random\n","\n","from models.experimental import attempt_load\n","from utils.datasets import LoadStreams, LoadImages\n","from utils.general import check_img_size, non_max_suppression, apply_classifier, scale_coords, xyxy2xywh, \\\n","    strip_optimizer, set_logging, increment_path\n","from utils.plots import plot_one_box\n","from utils.torch_utils import select_device, load_classifier, time_synchronized\n","\n","import glob\n","\n","def crop(dataset_dir='', model_path='/content/yolov3/runs/train/exp/best.pt'):\n","    save_txt, imgsz = False, 224\n","    weights = model_path\n","    projectP = 'runs/detect'\n","    projectNameP = 'exp'\n","    save_img = True\n","    view_img = True\n","\n","    save_dir = Path(increment_path(Path(projectP) / projectNameP, False))  # increment run\n","    (save_dir / 'labels' if save_txt else save_dir).mkdir(parents=True, exist_ok=True)  # make dir\n","\n","    #loop over train, val, and test set\n","    trainTestVarDirs = glob.glob(dataset_dir + \"*\")\n","    for setDir in trainTestVarDirs:\n","      splitDir = os.path.basename(setDir)\n","      setClasses = glob.glob(setDir + \"/*\")\n","      for setClass in setClasses:\n","        # Directories\n","        classDir = os.path.basename(setClass)\n","        finalSaveDir = os.path.join(save_dir, splitDir, classDir)\n","        Path(finalSaveDir).mkdir(parents=True, exist_ok=True)\n","        source = setClass\n","        \n","\n","        # Initialize\n","        set_logging()\n","        device = select_device('0')\n","        half = device.type != 'cpu'  # half precision only supported on CUDA\n","\n","        # Load model\n","        model = attempt_load(weights, map_location=device)  # load FP32 model\n","        imgsz = check_img_size(imgsz, s=model.stride.max())  # check img_size\n","        \n","        #introducing grid size\n","        gs = model.stride.max()\n","        #end\n","\n","        if half:\n","            model.half()  # to FP16\n","\n","        # Second-stage classifier\n","        classify = False\n","        if classify:\n","            modelc = load_classifier(name='resnet101', n=2)  # initialize\n","            modelc.load_state_dict(torch.load('weights/resnet101.pt', map_location=device)['model']).to(device).eval()\n","\n","        # Set Dataloader\n","        vid_path, vid_writer = None, None\n","        \n","        dataset = LoadImages(source, img_size=imgsz)\n","\n","        # Get names and colors\n","        names = model.module.names if hasattr(model, 'module') else model.names\n","        colors = [[random.randint(0, 255) for _ in range(3)] for _ in names]\n","\n","        colors = [[217, 175, 78]]\n","\n","        # Run inference\n","        t0 = time.time()\n","        img = torch.zeros((1, 3, imgsz, imgsz), device=device)  # init img\n","        _ = model(img.half() if half else img) if device.type != 'cpu' else None  # run once\n","        for path, img, im0s, vid_cap in dataset:\n","            img = torch.from_numpy(img).to(device)\n","            img = img.half() if half else img.float()  # uint8 to fp16/32\n","            img /= 255.0  # 0 - 255 to 0.0 - 1.0\n","            if img.ndimension() == 3:\n","                img = img.unsqueeze(0)\n","\n","            # Inference\n","            t1 = time_synchronized()\n","            pred = model(img, augment=True)[0]\n","\n","            # Apply NMS\n","            final_pred = non_max_suppression(pred, 0.15, 0.3, classes=0, agnostic=True)\n","            pred = non_max_suppression(pred, 0.00005, 1, classes=0, agnostic=True)\n","            t2 = time_synchronized()\n","\n","            # Apply Classifier\n","            if classify:\n","                pred = apply_classifier(pred, modelc, img, im0s)\n","\n","            # Process detections\n","            for i, det in enumerate(pred):  # detections per image\n","                \n","                p, s, im0 = Path(path), '', im0s.copy()\n","\n","                imoriginal = im0.copy()\n","                #plot grid\n","                numofsquares = int(imgsz/int(gs))\n","                rowstep = int(im0.shape[0]/numofsquares)\n","                colstep = int(im0.shape[1]/numofsquares)\n","                plot_borders(im0, line_color=(0,0,0), thickness=2)\n","                gridim_solo = im0.copy()\n","                plot_grid(gridim_solo, pxstep=rowstep, pystep=colstep, line_color=(0,0,0), thickness=2)\n","                #end plot grid\n","     \n","                save_path = str(finalSaveDir + \"/\" + p.name)\n","                s += '%gx%g ' % img.shape[2:]  # print string\n","                gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]  # normalization gain whwh\n","                if len(det):\n","                    # Rescale boxes from img_size to im0 size\n","                    det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0.shape).round()\n","\n","                    # Print results\n","                    for c in det[:, -1].unique():\n","                        n = (det[:, -1] == c).sum()  # detections per class\n","                        s += '%g %ss, ' % (n, names[int(c)])  # add to string\n","                    \n","                    # Write results\n","                    for *xyxy, conf, cls in reversed(det):\n","                        if save_img or view_img:  # Add bbox to image\n","                            label = ''#'%s %.2f' % (names[int(cls)], conf)\n","                            plot_one_box(xyxy, im0, label=label, color=colors[int(cls)], line_thickness=1)\n","                            \n","                \n","\n","                # Print time (inference + NMS)\n","                print('%sDone. (%.3fs)' % (s, t2 - t1))\n","\n","                # Save results (image with detections)\n","                if save_img:\n","                    cv2.imwrite(save_path + \"_original.jpg\", imoriginal)\n","                    cv2.imwrite(save_path, im0)\n","                    cv2.imwrite(save_path + \"_grid.jpg\", gridim_solo)\n","                        \n","\n","\n","            # SAVE FINAL CROPPED IMAGES\n","            # Process detections\n","            for i, det in enumerate(final_pred):  # detections per image\n","                \n","                p, s, im0 = Path(path), '', im0s\n","                im2 = im0.copy() #to use with grid/map\n","                #background\n","                numofsquares = int(imgsz/int(gs))\n","                rowstep = int(im0.shape[0]/numofsquares)\n","                colstep = int(im0.shape[1]/numofsquares)\n","                plot_overlay([0,0, im2.shape[1], im2.shape[0]], im2, color=(255, 255, 255), alpha=0.7, pxstep=rowstep, pystep=colstep)\n","                \n","                #borders\n","                plot_borders(im2, line_color=(0,0,0), thickness=2)\n","                plot_borders(im0, line_color=(0,0,0), thickness=2)\n","\n","                save_path = str(finalSaveDir + \"/\" +  p.name)\n","                s += '%gx%g ' % img.shape[2:]  # print string\n","                gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]  # normalization gain whwh\n","                if len(det):\n","                    # Rescale boxes from img_size to im0 size\n","                    det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0.shape).round()\n","\n","                    # Print results\n","                    for c in det[:, -1].unique():\n","                        n = (det[:, -1] == c).sum()  # detections per class\n","                        s += '%g %ss, ' % (n, names[int(c)])  # add to string\n","\n","                    #FUNCTION custom crop\n","                    CROP = True\n","                    if CROP:\n","                        fidx = 0\n","                        for *xyxy, conf, cls in reversed(det):\n","                            if save_img or view_img:\n","                                fidx = fidx + 1\n","                                crop_object(im0, xyxy, str(finalSaveDir + \"/\" +  (p.stem + \"_cropped_\" + str(fidx) + p.suffix)))\n","                    #END\n","                    \n","                    # Write results\n","                    for *xyxy, conf, cls in reversed(det):\n","                        if save_img or view_img:  # Add bbox to image\n","                            label = ''#'%s %.2f' % (names[int(cls)], conf)\n","                            plot_one_box(xyxy, im0, label=label, color=colors[int(cls)], line_thickness=2)\n","                            plot_overlay(xyxy, im2, color=colors[int(cls)], alpha=0.7, pxstep=rowstep, pystep=colstep)\n","                else:\n","                    cv2.imwrite(save_path + \"_not_cropped.jpg\", im0)\n","\n","\n","                gridim = im2.copy()\n","                plot_grid(gridim, pxstep=rowstep, pystep=colstep, line_color=(0,0,0), thickness=2)\n","                \n","                # Print time (inference + NMS)\n","                print('%sDone. (%.3fs)' % (s, t2 - t1))\n","\n","                # Save results (image with detections)\n","                if save_img:\n","                    cv2.imwrite(save_path + \"_map.jpg\", gridim)\n","                    cv2.imwrite(save_path + \"_final.jpg\", im0)\n","\n","        if save_txt or save_img:\n","            s = f\"\\n{len(list(finalSaveDir.glob('labels/*.txt')))} labels saved to {finalSaveDir + '/' + 'labels'}\" if save_txt else ''\n","            print(f\"Results saved to {finalSaveDir}{s}\")\n","\n","        print('Done. (%.3fs)' % (time.time() - t0))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WXcbhb9y3yag"},"source":["# Read Define prerequisite functions cell (double click on it).\n","# Call the crop function with its corresponding parameters. The pre-trained model is located under /content/models/weights/RSGAI_YOLOv3.pt."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Wp2Ej74byTL-"},"source":["# Solution"]},{"cell_type":"code","metadata":{"id":"JDxe16VQyV7-"},"source":["crop(dataset_dir='/content/dataset/split/', model_path='/content/models/weights/RSGAI_YOLOv3.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cqOjSM_Y-OEU"},"source":["# Preview a Cropped Image"]},{"cell_type":"code","metadata":{"cellView":"form","id":"XFm6eyhw-GJJ"},"source":["#@title Generate preview\n","\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","import matplotlib.image as mpimg\n","import os\n","import glob\n","lastExp = max(glob.glob(os.path.join('/content/yolov3/runs/detect','*/' )), key=os.path.getmtime)\n","\n","imgPath = lastExp + 'test/Tomato___Leaf_Mold/image (114).JPG'\n","oringinalImg = mpimg.imread(imgPath + \"_original.jpg\")\n","boundingBoxesImg = mpimg.imread(imgPath)\n","croppedImg = mpimg.imread(imgPath.replace(\".JPG\", \"_cropped_1.JPG\"))\n","gridImg = mpimg.imread(imgPath+ \"_grid.jpg\")\n","mapImg = mpimg.imread(imgPath+ \"_map.jpg\")\n","finaldetectImg = mpimg.imread(imgPath+ \"_final.jpg\")\n","\n","print(\"Original Image:\")\n","plt.axis('off')\n","plt.imshow(oringinalImg)\n","plt.show()\n","\n","print(\"Grid:\")\n","plt.axis('off')\n","plt.imshow(gridImg)\n","plt.show()\n","\n","print(\"Bounding Boxes:\")\n","plt.axis('off')\n","plt.imshow(boundingBoxesImg)\n","plt.show()\n","\n","print(\"Probability Map:\")\n","plt.axis('off')\n","plt.imshow(mapImg)\n","plt.show()\n","\n","print(\"Final Detection:\")\n","plt.axis('off')\n","plt.imshow(finaldetectImg)\n","plt.show()\n","\n","\n","print(\"Cropped Image:\")\n","plt.axis('off')\n","plt.imshow(croppedImg)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gyXcveLiiIcq"},"source":["# 4. Exercise III: Segmenting Leaf Images\n","\n","Before you start, make sure to run the \"Install and import prequisites\", \"Download pretrained model\", and \"Define prerequisite functions\" code cells.\n","Once you run all of them, you are required to pass the path of the folder containing the cropped tomato leaves to the segment function below. The cropped version of the dataset is located under /content/dataset/tomato-cropped/.\n","Running the segment function will trigger SegNet to segment the images using a pretrained model. \n","After that, you should be able to run the \"Preview a Segmented Image\" cell and see a sample image from the results."]},{"cell_type":"code","metadata":{"cellView":"form","id":"LyU8pyFwi7Of"},"source":["#@title Install and import prerequisites\n","!git clone https://github.com/divamgupta/image-segmentation-keras\n","%cd image-segmentation-keras\n","from keras_segmentation.models.segnet import segnet\n","print(\"Keras and SegNet are loaded\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"IbJsLl84jNRP"},"source":["#@title Download pretrained model\n","##########################\n","### DOWNLOAD THE MODEL ###\n","##########################\n","import requests\n","import os\n","import zipfile\n","## FEEL FREE TO CHANGE THESE PARAMETERS\n","model_URL = \"http://faridnakhle.com/pv/models/SegNet.zip\"\n","save_data_to = \"/content/models/\"\n","model_file_name = \"segnet.zip\"\n","#######################################\n","\n","if not os.path.exists(save_data_to):\n","    os.makedirs(save_data_to)\n","\n","print(\"Downloading model...\")  \n","\n","r = requests.get(model_URL, stream = True, headers={\"User-Agent\": \"Ready, Steady, Go AI\"})\n","with open(save_data_to + model_file_name, \"wb\") as file: \n","    for block in r.iter_content(chunk_size = 1024):\n","         if block: \n","             file.write(block)\n","\n","## Extract downloaded zip dataset file\n","print(\"Model downloaded\")  \n","print(\"Extracting files...\")\n","\n","with zipfile.ZipFile(save_data_to + model_file_name, 'r') as zip_dataset:\n","    zip_dataset.extractall(save_data_to)\n","print(\"Done!\")\n","\n","\n","## CROPPED DATASET\n","\n","## FEEL FREE TO CHANGE THESE PARAMETERS\n","dataset_url = \"http://faridnakhle.com/pv/tomato-split-cropped.zip\"\n","save_data_to = \"/content/dataset/tomato-cropped/\"\n","dataset_file_name = \"tomato-cropped.zip\"\n","#######################################\n","\n","if not os.path.exists(save_data_to):\n","    os.makedirs(save_data_to)\n","\n","r = requests.get(dataset_url, stream = True, headers={\"User-Agent\": \"Ready, Steady, Go AI\"})\n","\n","print(\"Downloading dataset...\")  \n","\n","with open(save_data_to + dataset_file_name, \"wb\") as file: \n","    for block in r.iter_content(chunk_size = 1024):\n","         if block: \n","             file.write(block)\n","\n","## Extract downloaded zip dataset file\n","print(\"Dataset downloaded\")  \n","print(\"Extracting files...\")  \n","with zipfile.ZipFile(save_data_to + dataset_file_name, 'r') as zip_dataset:\n","    zip_dataset.extractall(save_data_to)\n","\n","## Delete the zip file as we no longer need it\n","os.remove(save_data_to + dataset_file_name)\n","print(\"All done!\")  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"nk2PP9RWj1yr"},"source":["#@title Define prerequisite functions\n","\n","import cv2\n","import numpy as np\n","from keras_segmentation.models.segnet import segnet\n","import glob\n","import os\n","from tqdm import tqdm\n","import six\n","\n","def segment(inptDir = \"\"):\n","\n","  modelName = \"/content/models/RSGAI_SegNet.hdf5\"\n","  model = segnet(n_classes=50 ,  input_height=320, input_width=640)\n","  model.load_weights(modelName)\n","\n","  outputDir = \"/content/dataset/tomato-cropped-segmented/\"\n","\n","  inptDirGlob = glob.glob(inptDir + \"*\")\n","  for setDir in inptDirGlob:\n","\n","    splitDir = os.path.basename(setDir)\n","    setClasses = glob.glob(setDir + \"/*\")\n","\n","    for setClass in setClasses:\n","\n","      classDir = os.path.basename(setClass)\n","      inptFolder = os.path.join(inptDir, splitDir, classDir)\n","      outputFolder = os.path.join(outputDir, splitDir, classDir)\n","\n","      if not os.path.exists(outputFolder):\n","          os.makedirs(outputFolder)\n","\n","      inps = glob.glob(os.path.join(inptFolder, \"*.jpg\")) + glob.glob(\n","          os.path.join(inptFolder, \"*.png\")) + \\\n","          glob.glob(os.path.join(inptFolder, \"*.jpeg\"))+ \\\n","          glob.glob(os.path.join(inptFolder, \"*.JPG\"))\n","      inps = sorted(inps)\n","\n","      if len(inps) > 0:\n","\n","        all_prs = []\n","\n","        for i, inp in enumerate(tqdm(inps)):\n","            if outputFolder is None:\n","                out_fname = None\n","            else:\n","                if isinstance(inp, six.string_types):\n","                    out_fname = os.path.join(outputFolder, os.path.basename(inp))\n","                else:\n","                    out_fname = os.path.join(outputFolder, str(i) + \".jpg\")\n","\n","            pr = model.predict_segmentation(\n","                inp=inp,\n","                out_fname=out_fname\n","            )\n","\n","            img = cv2.imread(inp)\n","            seg = cv2.imread(out_fname)\n","\n","            for row in range(0, len(seg)):\n","                for col in range(0, len(seg[0])):\n","                    #if np.all(seg[row, col] == [7,47,204]) == False:\n","                    #    img[row, col] = [0,0,0]\n","                    \n","                    if seg[row, col][0] > 50:\n","                        img[row, col] = [0,0,0]\n","            all_prs.append(pr)\n","            cv2.imwrite(out_fname, img)\n","  print(\"Segmented images are saved in:\") \n","  print(outputDir)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R3LcyzvFk3YD"},"source":["# Read Define prerequisite functions cell (double click on it).\n","# Call the segment function with its corresponding parameters."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c4lR3qZZluTM"},"source":["# Solution"]},{"cell_type":"code","metadata":{"id":"w4qNAmFVls9-"},"source":["segment(inptDir=\"/content/dataset/tomato-cropped/\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q0gQLzqzmzeb"},"source":["# Preview a Segmented Image"]},{"cell_type":"code","metadata":{"cellView":"form","id":"STmIaG74mWJN"},"source":["#@title Generate Preview\n","import matplotlib.pyplot as plt\n","import matplotlib.image as mpimg\n","imgPath = '/content/dataset/tomato-cropped/test/Tomato___Septoria_leaf_spot/image (1020)_cropped_1.JPG'\n","segmemtedPath = '/content/dataset/tomato-cropped-segmented/test/Tomato___Septoria_leaf_spot/image (1020)_cropped_1.JPG'\n","\n","oringinalImg = mpimg.imread(imgPath)\n","segmentedImage = mpimg.imread(segmemtedPath)\n","\n","print(\"Original Image:\")\n","plt.axis('off')\n","plt.imshow(oringinalImg)\n","plt.show()\n","\n","print(\"Segmented Image:\")\n","plt.axis('off')\n","plt.imshow(segmentedImage)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i6BqFC8onFQM"},"source":["# 5. Exercise IV: Descriptive Data Analysis\n","\n","Before you start, make sure to run the \"Install and import prequisites\" code cell. \n","\n","Next, loop over the classes in the segmented training data folder located under /content/dataset/tomato-segmented/, then count the images in each. You can use matplotlib to generate the plot.\n","\n","Based on the above, you can decide whether or not there is a need for data balancing."]},{"cell_type":"code","metadata":{"id":"1Qd3aYENnEMS","cellView":"form"},"source":["#@title Install and import prequisites\n","\n","import requests\n","import os\n","import zipfile\n","\n","## FEEL FREE TO CHANGE THESE PARAMETERS\n","dataset_url = \"http://faridnakhle.com/pv/tomato-split-cropped-segmented.zip\"\n","save_data_to = \"/content/dataset/tomato-segmented/\"\n","dataset_file_name = \"tomato-segmented.zip\"\n","#######################################\n","\n","if not os.path.exists(save_data_to):\n","    os.makedirs(save_data_to)\n","\n","r = requests.get(dataset_url, stream = True, headers={\"User-Agent\": \"Ready, Steady, Go AI\"})\n","\n","print(\"Downloading dataset...\")  \n","\n","with open(save_data_to + dataset_file_name, \"wb\") as file: \n","    for block in r.iter_content(chunk_size = 1024):\n","         if block: \n","             file.write(block)\n","\n","## Extract downloaded zip dataset file\n","print(\"Dataset downloaded\")  \n","print(\"Extracting files...\")  \n","with zipfile.ZipFile(save_data_to + dataset_file_name, 'r') as zip_dataset:\n","    zip_dataset.extractall(save_data_to)\n","\n","## Delete the zip file as we no longer need it\n","os.remove(save_data_to + dataset_file_name)\n","print(\"All done!\")  \n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UmXZ2ag_n2DT"},"source":["### WRITE YOUR CODE HERE ###"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wgw_WVucp1l5"},"source":["# Solution"]},{"cell_type":"code","metadata":{"id":"8zD6vDw5oWiQ"},"source":["import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import os\n","import shutil\n","import cv2\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","train_dir = '/content/dataset/tomato-segmented/train/'\n","train_classes = [path for path in os.listdir(train_dir)]\n","train_imgs = dict([(ID, os.listdir(os.path.join(train_dir, ID))) for ID in train_classes])\n","train_classes_count = []\n","for trainClass in train_classes:\n","  train_classes_count.append(len(train_imgs[trainClass]))\n","  \n","plt.figure(figsize=(15, 10))\n","g = sns.barplot(x=train_classes, y=train_classes_count)\n","g.set_xticklabels(labels=train_classes, rotation=30, ha='right')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HEHo-wEkL35G"},"source":["# 6. Exercise V: Balancing the Dataset\n","\n","Based on the results of the Descriptive Data Analysis (exercise IV), it can be seen that that the classes are imbalanced. In this exercise, you are required to:\n","\n","**A.** Use Augmentor to oversample any class containing less than 1500 images, except for the healthy class.\n","\n","**B.** Use DCGAN to synthesize images for the healthy class. You need to point the syntesizing function to the path of the pretrained model. The path is saved under /content/models/RSGAI_DCGAN.pth\n","\n","**C.** Use KNN to reduce the yellow leaf curl virus class.\n","\n","Balanced classes should end up with 1500 images each.\n"," \n"," **NB:** After data balancing, generate the data distribution plot again to analyze the new distribution of classes.\n","\n"," Make sure to run the \"Install and import prequisites\", \"Download pretrained models\", and \"Define prerequisite functions\" cells first."]},{"cell_type":"code","metadata":{"cellView":"form","id":"MXTFT5q4NRC9"},"source":["#@title Install and import prequisites\n","\n","!pip install Augmentor\n","import Augmentor\n","import os\n","\n","def makedir(path):\n","    '''\n","    if path does not exist in the file system, create it\n","    '''\n","    if not os.path.exists(path):\n","        os.makedirs(path)\n","\n","datasets_root_dir = '/content/dataset/tomato-segmented/'\n","dir = datasets_root_dir + 'train/'\n","target_dir = dir #same directory as input\n","makedir(target_dir)\n","\n","folders = [os.path.join(dir, folder) for folder in next(os.walk(dir))[1]]\n","target_folders = [os.path.join(target_dir, folder) for folder in next(os.walk(dir))[1]]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"CHMPJ3QCTNyS"},"source":["#@title Download pretrained models\n","\n","##########################\n","### DOWNLOAD THE MODEL ###\n","##########################\n","\n","## FEEL FREE TO CHANGE THESE PARAMETERS\n","model_URL = \"http://faridnakhle.com/pv/models/RSGAI_DCGAN.zip\"\n","save_data_to = \"/content/models/\"\n","model_file_name = \"dcgan.zip\"\n","#######################################\n","\n","if not os.path.exists(save_data_to):\n","    os.makedirs(save_data_to)\n","\n","print(\"Downloading model...\")  \n","\n","r = requests.get(model_URL, stream = True, headers={\"User-Agent\": \"Ready, Steady, Go AI\"})\n","with open(save_data_to + model_file_name, \"wb\") as file: \n","    for block in r.iter_content(chunk_size = 1024):\n","         if block: \n","             file.write(block)\n","\n","## Extract downloaded zip dataset file\n","print(\"Model downloaded\")  \n","print(\"Extracting files...\")\n","\n","with zipfile.ZipFile(save_data_to + model_file_name, 'r') as zip_dataset:\n","    zip_dataset.extractall(save_data_to)\n","print(\"All done!\")  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"EH855sRkTcKM"},"source":["#@title Define prerequisite functions\n","import argparse\n","import os\n","import numpy as np\n","import math\n","\n","import torchvision.transforms as transforms\n","from torchvision.utils import save_image\n","\n","from torch.utils.data import DataLoader\n","from torchvision import datasets\n","from torch.autograd import Variable\n","\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch\n","## YOU CAN CHANGE THESE VARIABLES    \n","n_epochs = 300\n","batch_size = 50\n","lr = 0.0002\n","b1 = 0.7 #adam: decay of first order momentum of gradient\n","b2 = 0.999 #adam: decay of first order momentum of gradient\n","n_cpu = 1\n","latent_dim = 100 #dimensionality of the latent space\n","img_size = 224\n","channels = 3 #R, G, and B\n","sample_interval = 400 #interval between image sampling\n","######################################################\n","\n","class Generator(nn.Module):\n","    def __init__(self):\n","        super(Generator, self).__init__()\n","\n","        self.init_size = img_size // 4\n","        self.l1 = nn.Sequential(nn.Linear(latent_dim, 128 * self.init_size ** 2))\n","\n","        self.conv_blocks = nn.Sequential(\n","            nn.BatchNorm2d(128),\n","            nn.Upsample(scale_factor=2),\n","            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n","            nn.BatchNorm2d(128, 0.8),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.Upsample(scale_factor=2),\n","            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n","            nn.BatchNorm2d(64, 0.8),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.Conv2d(64, channels, 3, stride=1, padding=1),\n","            nn.Tanh(),\n","        )\n","\n","    def forward(self, z):\n","        out = self.l1(z)\n","        out = out.view(out.shape[0], 128, self.init_size, self.init_size)\n","        img = self.conv_blocks(out)\n","        return img\n","\n","class Discriminator(nn.Module):\n","    def __init__(self):\n","        super(Discriminator, self).__init__()\n","\n","        def discriminator_block(in_filters, out_filters, bn=True):\n","            block = [nn.Conv2d(in_filters, out_filters, 3, 2, 1), nn.LeakyReLU(0.2, inplace=True), nn.Dropout2d(0.25)]\n","            if bn:\n","                block.append(nn.BatchNorm2d(out_filters, 0.8))\n","            return block\n","\n","        self.model = nn.Sequential(\n","            *discriminator_block(channels, 16, bn=False),\n","            *discriminator_block(16, 32),\n","            *discriminator_block(32, 64),\n","            *discriminator_block(64, 128),\n","        )\n","\n","        # The height and width of downsampled image\n","        ds_size = img_size // 2 ** 4\n","        self.adv_layer = nn.Sequential(nn.Linear(128 * ds_size ** 2, 1), nn.Sigmoid())\n","\n","    def forward(self, img):\n","        out = self.model(img)\n","        out = out.view(out.shape[0], -1)\n","        validity = self.adv_layer(out)\n","\n","        return validity\n","\n","def GenerateImages(modelPath, outPutFolder, IMGS2GENERATE):\n","   \n","    if not os.path.exists(outPutFolder):\n","        os.makedirs(outPutFolder)\n","\n","    ## YOU CAN CHANGE THESE VARIABLES    \n","    n_epochs = 1\n","    batch_size = 50\n","    lr = 0.0002\n","    b1 = 0.7 #adam: decay of first order momentum of gradient\n","    b2 = 0.999 #adam: decay of first order momentum of gradient\n","    n_cpu = 1\n","    latent_dim = 100 #dimensionality of the latent space\n","    img_size = 224\n","    channels = 3 #R, G, and B\n","    sample_interval = 400 #interval between image sampling\n","    ######################################################\n","\n","    def weights_init_normal(m):\n","        classname = m.__class__.__name__\n","        if classname.find(\"Conv\") != -1:\n","            torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n","        elif classname.find(\"BatchNorm2d\") != -1:\n","            torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n","            torch.nn.init.constant_(m.bias.data, 0.0)\n","\n","    cuda = True if torch.cuda.is_available() else False\n","\n","    load_from_checkpoint = True\n","\n","    # Loss function\n","    adversarial_loss = torch.nn.BCELoss()\n","\n","    # Initialize generator and discriminator\n","    generator = Generator()\n","    discriminator = Discriminator()\n","\n","    if cuda:\n","        generator.cuda()\n","        discriminator.cuda()\n","        adversarial_loss.cuda()\n","\n","    # Initialize weights\n","    generator.apply(weights_init_normal)\n","    discriminator.apply(weights_init_normal)\n","\n","    # Optimizers\n","    optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(b1, b2))\n","    optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(b1, b2))\n","\n","    Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n","\n","    # ----------\n","    #  Load from Checkpoint\n","    # ----------\n","\n","    if (load_from_checkpoint):\n","        checkpointName = modelPath\n","        checkpoint = torch.load(checkpointName)\n","        generator.load_state_dict(checkpoint['G_state_dict'])\n","        discriminator.load_state_dict(checkpoint['D_state_dict'])\n","        optimizer_G.load_state_dict(checkpoint['G_optimizer'])\n","        optimizer_D.load_state_dict(checkpoint['D_optimizer'])\n","        print(\"Loaded CheckPoint: \" + checkpointName)\n","        if cuda:\n","            generator.cuda()\n","            discriminator.cuda()\n","\n","    # ----------\n","    #  Generating images\n","    # ----------\n","\n","    for i in range (0, IMGS2GENERATE):\n","        z = Variable(Tensor(np.random.normal(0, 1, (1, latent_dim))))\n","        # Generate a batch of images\n","        gen_imgs = generator(z)\n","        save_image(gen_imgs.data, outPutFolder + \"/DCGAN_%d.png\" % (i + 1), nrow=0, normalize=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AViHJNEeQpck"},"source":["# A. Augmentor"]},{"cell_type":"code","metadata":{"id":"X3s2wtQXQVO3"},"source":["## Creante an Augmentor Pipline ##\n","## Do not augment the healthy class ##"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FqoG0D16QxhL"},"source":["# Solution"]},{"cell_type":"code","metadata":{"id":"YgMgWADkPpDK"},"source":["requiredNbrOfImages = 1500\n","\n","for i in range(len(folders)):\n","    if folders[i].endswith(\"healthy\") == False:\n","        path, dirs, files = next(os.walk(folders[i]))\n","        nbrOfImages = len(files)\n","        nbrOfImagesNeeded = requiredNbrOfImages - nbrOfImages\n","          \n","        if nbrOfImagesNeeded > 0:\n","            tfd = target_folders[i]\n","            print (\"saving in \" + tfd)\n","            p = Augmentor.Pipeline(source_directory=folders[i], output_directory=tfd)\n","            p.rotate(probability=1, max_left_rotation=15, max_right_rotation=15)\n","            p.flip_left_right(probability=0.5)\n","            p.skew(probability=1, magnitude=0.2)\n","            p.flip_left_right(probability=0.5)\n","            p.shear(probability=1, max_shear_left=10, max_shear_right=10)\n","            p.flip_left_right(probability=0.5)\n","            p.sample(nbrOfImagesNeeded)\n","print(\"Dataset Augmented!\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mzm2brylUJI7"},"source":["# B. DCGAN"]},{"cell_type":"code","metadata":{"id":"h_0RTodLU7ce"},"source":["## Use the GenerateImages() function. Hint: IMGS2GENERATE variable should be 228."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h14kFcdUVF8X"},"source":["# Solution"]},{"cell_type":"code","metadata":{"id":"CkBT_el3UHqI"},"source":["GenerateImages('/content/models/RSGAI_DCGAN.pth', '/content/dataset/dcgan/', IMGS2GENERATE = 228)\n","print(\"Data Generated\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JMemfw_dVyD4"},"source":["# Preview Some Generated Images\n"]},{"cell_type":"code","metadata":{"cellView":"form","id":"y9IMahYZV6PB"},"source":["#@title Preview some syntetic images\n","\n","import matplotlib.pyplot as plt\n","import matplotlib.image as mpimg\n","imgPath = '/content/dataset/dcgan/DCGAN_'\n","imageOne = mpimg.imread(imgPath + \"1.png\")\n","imageTen = mpimg.imread(imgPath + \"10.png\")\n","\n","plt.axis('off')\n","plt.imshow(imageOne)\n","plt.show()\n","\n","plt.axis('off')\n","plt.imshow(imageTen)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SjvP6WuPWSoI"},"source":["# C. KNN"]},{"cell_type":"code","metadata":{"cellView":"form","id":"W8TVZh3mWXxE"},"source":["#@title Load prerequisites\n","\n","from sklearn.neighbors import NearestNeighbors\n","from glob import glob\n","\n","import numpy as np\n","import scipy.sparse as sp\n","from keras.applications import VGG19\n","from keras.applications.vgg19 import preprocess_input\n","from keras.engine import Model\n","from keras.preprocessing import image\n","import numpy as np\n","import os\n","deleteImages = True\n","def SaveFile(arr, filename):\n","    with open(filename, 'w') as filehandle:\n","        for listitem in arr:\n","            filehandle.write(str(listitem) + \"\\n\")\n","\n","\n","def vectorize_all(files, model, px=224, n_dims=512, batch_size=512):\n","    min_idx = 0\n","    max_idx = min_idx + batch_size\n","    total_max = len(files)\n","    if (max_idx > total_max):\n","        max_idx = total_max\n","    \n","    preds = sp.lil_matrix((len(files), n_dims))\n","\n","    print(\"Total: {}\".format(len(files)))\n","    while min_idx < total_max - 1:\n","        print(min_idx)\n","        X = np.zeros(((max_idx - min_idx), px, px, 3))\n","        # For each file in batch, \n","        # load as row into X\n","        i = 0\n","        for i in range(min_idx, max_idx):\n","            file = files[i]\n","            try:\n","                img = image.load_img(file, target_size=(px, px))\n","                img_array = image.img_to_array(img)\n","                X[i - min_idx, :, :, :] = img_array\n","            except Exception as e:\n","                print(e)\n","        max_idx = i\n","        X = preprocess_input(X)\n","        these_preds = model.predict(X)\n","        shp = ((max_idx - min_idx) + 1, n_dims)\n","        preds[min_idx:max_idx + 1, :] = these_preds.reshape(shp)\n","        min_idx = max_idx\n","        max_idx = np.min((max_idx + batch_size, total_max))\n","    return preds\n","\n","def vectorizeOne(path, model):\n","    img = image.load_img(path, target_size=(224, 224))\n","    x = image.img_to_array(img)\n","    x = np.expand_dims(x, axis=0)\n","    x = preprocess_input(x)\n","    pred = model.predict(x)\n","    return pred.ravel()\n","\n","def findSimilar(vec, knn, filenames, n_neighbors=6):\n","    if n_neighbors >= len(filenames):\n","        print(\"Error. number of neighbours should be less than the number of images.\")\n","    else:\n","        n_neighbors = n_neighbors + 1\n","        dist, indices = knn.kneighbors(vec.reshape(1, -1), n_neighbors=n_neighbors)\n","        dist, indices = dist.flatten(), indices.flatten()\n","        similarList = [(filenames[indices[i]], dist[i]) for i in range(len(indices))]\n","        del similarList[0]\n","        #similarImages.sort(reverse=True, key=lambda tup: tup[1])\n","        return similarList"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XzqTTodlW9c3"},"source":["## REPLACE ? WITH THE NAME OF THE CLASS TO BE DOWNSAMPLED\n","img_dir = \"/content/dataset/tomato-segmented/train/?/*\"\n","targetLimit = ? ## REPLACE ? WITH THE DESIRED NUMBER OF IMAGES\n","\n","files = glob(img_dir)\n","\n","nbrOfImages2Delete = len(files) - targetLimit\n","\n","if (nbrOfImages2Delete > 0):\n","\n","    imgToSearchFor = files[0]\n","    base_model = VGG19(weights='imagenet')\n","    model = Model(inputs=base_model.input, outputs=base_model.get_layer('fc1').output)\n","    vecs = vectorize_all(files, model, n_dims=4096)\n","\n","    ######################\n","    ### YOUR CODE HERE ###\n","    ## Create a variable named knn and assign it to a KNN model using \"cosine\" as metric and \"brute\" as the algorithm  variable.\n","    ## Fit the KNN model with vecs variable.\n","    ###############################\n","    \n","    vec = vectorizeOne(imgToSearchFor, model)\n","    similarImages = findSimilar(vec, knn, files, nbrOfImages2Delete)\n","    print(similarImages)\n","    SaveFile(similarImages, \"deletedImages.txt\")\n","\n","    if deleteImages:\n","        for i in range(0, len(similarImages)):\n","            if os.path.exists(similarImages[i][0]):\n","                os.remove(similarImages[i][0])\n","    print(\"Balancing done. A list of deleted images can be found in deletedImages.txt\")\n","else:\n","    print(\"nothing to delete\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n4HyqB-BW4xQ"},"source":["# Solution"]},{"cell_type":"code","metadata":{"id":"5hz63GcwWwdI"},"source":["img_dir = \"/content/dataset/tomato-segmented/train/Tomato___Tomato_Yellow_Leaf_Curl_Virus/*\"\n","targetLimit = 1500\n","deleteImages = True\n","\n","files = glob(img_dir)\n","nbrOfImages2Delete = len(files) - targetLimit\n","\n","if (nbrOfImages2Delete > 0):\n","\n","    imgToSearchFor = files[0]\n","\n","    base_model = VGG19(weights='imagenet')\n","    model = Model(inputs=base_model.input, outputs=base_model.get_layer('fc1').output)\n","    vecs = vectorize_all(files, model, n_dims=4096)\n","\n","    knn = NearestNeighbors(metric='cosine', algorithm='brute')\n","    knn.fit(vecs)\n","\n","    vec = vectorizeOne(imgToSearchFor, model)\n","    similarImages = findSimilar(vec, knn, files, nbrOfImages2Delete)\n","    SaveFile(similarImages, \"deletedImages.txt\")\n","\n","    if deleteImages:\n","        for i in range(0, len(similarImages)):\n","            if os.path.exists(similarImages[i][0]):\n","                os.remove(similarImages[i][0])\n","    print(\"Balancing done. A list of deleted images can be found in deletedImages.txt\")\n","else:\n","    print(\"nothing to delete\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FTVcB_sMaK9m"},"source":["# Display Final Data Distribution"]},{"cell_type":"code","metadata":{"cellView":"form","id":"NGOMd_XhaKua"},"source":["#@title Generate Data Distribution\n","\n","import requests\n","import os\n","import zipfile\n","\n","## FEEL FREE TO CHANGE THESE PARAMETERS\n","dataset_url = \"http://faridnakhle.com/pv/tomato-split-cropped-segmented-balanced.zip\"\n","save_data_to = \"/content/dataset/tomato-dataset-final/\"\n","dataset_file_name = \"tomato-split-cropped-segmented-balanced.zip\"\n","#######################################\n","\n","if not os.path.exists(save_data_to):\n","    os.makedirs(save_data_to)\n","\n","r = requests.get(dataset_url, stream = True, headers={\"User-Agent\": \"Ready, Steady, Go AI\"})\n","\n","print(\"Downloading dataset...\")  \n","\n","with open(save_data_to + dataset_file_name, \"wb\") as file: \n","    for block in r.iter_content(chunk_size = 1024):\n","         if block: \n","             file.write(block)\n","\n","## Extract downloaded zip dataset file\n","print(\"Dataset downloaded\")  \n","print(\"Extracting files...\")  \n","with zipfile.ZipFile(save_data_to + dataset_file_name, 'r') as zip_dataset:\n","    zip_dataset.extractall(save_data_to)\n","\n","## Delete the zip file as we no longer need it\n","os.remove(save_data_to + dataset_file_name)\n","print(\"All done!\")  \n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import os\n","import shutil\n","import cv2\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","train_dir = '/content/dataset/tomato-dataset-final/train/'\n","train_classes = [path for path in os.listdir(train_dir)]\n","train_imgs = dict([(ID, os.listdir(os.path.join(train_dir, ID))) for ID in train_classes])\n","train_classes_count = []\n","for trainClass in train_classes:\n","  train_classes_count.append(len(train_imgs[trainClass]))\n","\n","plt.figure(figsize=(15, 10))\n","g = sns.barplot(x=train_classes, y=train_classes_count)\n","g.set_xticklabels(labels=train_classes, rotation=30, ha='right')\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xcfN0hqtbeVy"},"source":["# 7. Exercise VI: Classification Using DenseNet-161 Pretrained DCNN algorithm"]},{"cell_type":"markdown","metadata":{"id":"-NDTlsBgbxlC"},"source":["In this exercise, you are required to load a pretrained DCNN model and test it with the testing dataset located under /dataset/tomato-dataset-final/test/ ."]},{"cell_type":"code","metadata":{"id":"yY1TnKkebxE4","cellView":"form"},"source":["#@title Load prerequisites and define needed functions\n","\n","import argparse\n","import os\n","import time\n","\n","import matplotlib.pyplot as plt\n","\n","import torch\n","import numpy as np\n","from torch import nn\n","from torch import optim\n","import torch.nn.functional as F\n","from torchvision import datasets, transforms, models\n","from PIL import Image\n","from collections import OrderedDict\n","import json\n","!pip install lime\n","from lime import lime_image\n","\n","## YOU CAN CHANGE THESE VARIABLES    \n","EPOCHS = 100\n","BATCH_SIZE = 20\n","LEARNING_RATE = 0.0001\n","data_dir = '/content/dataset/tomato-dataset-final/'\n","save_checkpoints = True\n","save_model_to = '/content/output/'\n","!mkdir /content/output/\n","IMG_SIZE = 220\n","NUM_WORKERS = 1\n","using_gpu = torch.cuda.is_available()\n","print_every = 300\n","ARCH = 'densenet161'\n","######################################################\n","\n","def data_loader(root, batch_size=256, workers=1, pin_memory=True):\n","    traindir = os.path.join(root, 'train')\n","    valdir = os.path.join(root, 'val')\n","    testdir = os.path.join(root, 'test')\n","    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                                     std=[0.229, 0.224, 0.225])\n","\n","    train_dataset = datasets.ImageFolder(\n","        traindir,\n","        transforms.Compose([\n","            transforms.Resize(size=(IMG_SIZE, IMG_SIZE)),\n","            transforms.ToTensor(),\n","            normalize\n","        ])\n","    )\n","    val_dataset = datasets.ImageFolder(\n","        valdir,\n","        transforms.Compose([\n","            transforms.Resize(size=(IMG_SIZE, IMG_SIZE)),\n","            transforms.ToTensor(),\n","            normalize\n","        ])\n","    )\n","    test_dataset = datasets.ImageFolder(\n","        testdir,\n","        transforms.Compose([\n","            transforms.Resize(size=(IMG_SIZE, IMG_SIZE)),\n","            transforms.ToTensor(),\n","            normalize\n","        ])\n","    )\n","\n","    train_loader = torch.utils.data.DataLoader(\n","        train_dataset,\n","        batch_size=batch_size,\n","        shuffle=True,\n","        num_workers=workers,\n","        pin_memory=pin_memory,\n","        sampler=None\n","    )\n","    val_loader = torch.utils.data.DataLoader(\n","        val_dataset,\n","        batch_size=batch_size,\n","        shuffle=False,\n","        num_workers=workers,\n","        pin_memory=pin_memory\n","    )\n","    test_loader = torch.utils.data.DataLoader(\n","        test_dataset,\n","        batch_size=batch_size,\n","        shuffle=False,\n","        num_workers=workers,\n","        pin_memory=pin_memory\n","    )\n","    return train_loader, val_loader, test_loader, train_dataset, val_dataset, test_dataset\n","\n","# Data loading\n","train_loader, val_loader, test_loader, train_dataset, val_dataset, test_dataset = data_loader(data_dir, BATCH_SIZE, NUM_WORKERS, False)\n","print(\"Training Set: \" + str(len(train_loader.dataset)))\n","print(\"Validation Set: \" + str(len(val_loader.dataset)))\n","print(\"Testing Set: \" + str(len(test_loader.dataset)))\n","\n","\n","##########################\n","### DOWNLOAD THE MODEL ###\n","##########################\n","\n","## FEEL FREE TO CHANGE THESE PARAMETERS\n","model_URL = \"http://faridnakhle.com/pv/models/RSGAI_DenseNet.zip\"\n","save_data_to = \"/content/models/\"\n","model_file_name = \"densenet.zip\"\n","#######################################\n","\n","if not os.path.exists(save_data_to):\n","    os.makedirs(save_data_to)\n","\n","print(\"Downloading model...\")  \n","\n","r = requests.get(model_URL, stream = True, headers={\"User-Agent\": \"Ready, Steady, Go AI\"})\n","with open(save_data_to + model_file_name, \"wb\") as file: \n","    for block in r.iter_content(chunk_size = 1024):\n","         if block: \n","             file.write(block)\n","\n","## Extract downloaded zip dataset file\n","print(\"Model downloaded\")  \n","print(\"Extracting files...\")\n","\n","with zipfile.ZipFile(save_data_to + model_file_name, 'r') as zip_dataset:\n","    zip_dataset.extractall(save_data_to)\n","print(\"All done!\")  \n","\n","\n","\n","\n","# Freeze parameters so we don't backprop through them\n","hidden_layers = [10240, 1024]\n","def make_model(structure, hidden_layers, lr, preTrained):\n","    if structure==\"densenet161\":\n","        model = models.densenet161(pretrained=preTrained)\n","        input_size = 2208\n","    else:\n","        model = models.vgg16(pretrained=preTrained)\n","        input_size = 25088\n","    output_size = 102\n","    for param in model.parameters():\n","        param.requires_grad = False\n","\n","    classifier = nn.Sequential(OrderedDict([\n","                              ('dropout',nn.Dropout(0.5)),\n","                              ('fc1', nn.Linear(input_size, hidden_layers[0])),\n","                              ('relu1', nn.ReLU()),\n","                              ('fc2', nn.Linear(hidden_layers[0], hidden_layers[1])),\n","                              ('relu2', nn.ReLU()),\n","                              ('fc3', nn.Linear(hidden_layers[1], output_size)),\n","                              ('output', nn.LogSoftmax(dim=1))\n","                              ]))\n","\n","    model.classifier = classifier\n","    return model\n","\n","model = make_model(ARCH, hidden_layers, LEARNING_RATE, True)\n","# define loss and optimizer\n","criterion = nn.NLLLoss()\n","optimizer = optim.Adam(model.classifier.parameters(), lr=LEARNING_RATE)\n","\n","def cal_accuracy(model, dataloader):\n","    validation_loss = 0\n","    accuracy = 0\n","    for i, (inputs,labels) in enumerate(dataloader):\n","                optimizer.zero_grad()\n","                inputs, labels = inputs.to('cuda') , labels.to('cuda')\n","                model.to('cuda')\n","                with torch.no_grad():    \n","                    outputs = model.forward(inputs)\n","                    validation_loss = criterion(outputs,labels)\n","                    ps = torch.exp(outputs).data\n","                    equality = (labels.data == ps.max(1)[1])\n","                    accuracy += equality.type_as(torch.FloatTensor()).mean()\n","                    \n","    validation_loss = validation_loss / len(dataloader)\n","    accuracy = accuracy /len(dataloader)\n","    \n","    return validation_loss, accuracy\n","\n","\n","\n","RESUME = True\n","RESUME_PATH ='/content/models/RSGAI_DenseNet.pth'\n","def loading_checkpoint(path):\n","    # Loading the parameters\n","    state = torch.load(path)\n","    LEARNING_RATE = state['learning_rate']\n","    structure = state['structure']\n","    hidden_layers = state['hidden_layers']\n","    epochs = state['epochs']\n","    \n","    # Building the model from checkpoints\n","    model = make_model(structure, hidden_layers, LEARNING_RATE, False) # IF NOT PRETRAINED CHANGE TO FALSE\n","    model.class_to_idx = state['class_to_idx']\n","    model.load_state_dict(state['state_dict'])\n","    model.eval()\n","    return model\n","\n","if RESUME:\n","  print(RESUME_PATH)\n","  if os.path.isfile(RESUME_PATH):\n","      model = loading_checkpoint(RESUME_PATH)\n","      print(\"=> loaded checkpoint '{}'\".format(RESUME_PATH))\n","  else:\n","    print(\"Invalid model Path\")\n","\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.cuda()\n","@torch.no_grad()\n","def get_all_preds(model, dataloader):\n","        \n","        all_preds = torch.tensor([])\n","        all_preds = all_preds.to(device)\n","        all_labels = torch.tensor([])\n","        all_labels = all_labels.to(device)\n","\n","        for data, target in dataloader:\n","            input = data.to(device)\n","            target = target.to(device)\n","\n","            with torch.no_grad():\n","                output = model(input)\n","\n","            all_preds = torch.cat(\n","                (all_preds, output)\n","                ,dim=0\n","            )\n","            all_labels = torch.cat(\n","                (all_labels, target)\n","                ,dim=0\n","            )\n","\n","        return all_preds, all_labels\n","    \n","def get_num_correct(preds, labels):\n","        return preds.argmax(dim=1).eq(labels).sum().item()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nxh2mR9vbowR"},"source":["with torch.no_grad():\n","    ## TEST THE MODEL ACCURACY USING THE TEST SET##"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iA3K-MgHdvIk"},"source":["# Solution"]},{"cell_type":"code","metadata":{"id":"iw4jLpf0dsaC"},"source":["with torch.no_grad():\n","    model.eval()\n","    test_preds, test_labels = get_all_preds(model,test_loader)\n","    preds_correct = get_num_correct(test_preds.cuda(), test_labels.cuda())\n","    print('total correct:', preds_correct)\n","    print('accuracy:')\n","    print(((preds_correct / (len(test_loader.dataset))) * 100))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kjFcCtveeog6"},"source":["# 8. Exercise VII: Generating Confusion Matrix\n","\n","In this exercise, you will plot the confusion matrix to visualize the prediction performance for each class."]},{"cell_type":"code","metadata":{"cellView":"form","id":"hfLUHSvDe2OX"},"source":["#@title Defining required functions\n","\n","def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n","    if normalize:\n","        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n","        #percentage: \n","        cm = cm.astype('float') * 100\n","        # add percentage sign\n","\n","    mycm = plt.imshow(cm, interpolation='nearest', cmap=cmap)\n","    mycm.set_clim([0,100])\n","    cbar = plt.colorbar(mycm, shrink=0.82, ticks=list(range(0, 120, 20)))\n","    cbar.ax.set_yticklabels(['0', '20', '40', '60', '80', '100'])  # vertically oriented colorbar\n","\n","    tick_marks = np.arange(len(classes))\n","    plt.xticks(tick_marks, classes, rotation=45,  ha=\"right\")\n","    \n","    plt.yticks(tick_marks, classes)\n","\n","    fmt = '.2f' if normalize else 'd'\n","    thresh = cm.max() / 2.\n","    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n","        plt.text(j, i, str(format(cm[i, j], fmt)) + \"%\", horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n","        \n","\n","    plt.rcParams['font.family'] = \"sans-serif\"\n","    plt.rcParams['font.sans-serif'] = \"Arial\"\n","    plt.rcParams.update({'font.size': 12})\n","    plt.ylabel('True class', fontsize=17, fontweight='bold')\n","    plt.xlabel('Predicted class', fontsize=17, fontweight='bold')\n","\n","import itertools\n","\n","cmt = torch.zeros(10, 10, dtype=torch.int32) #10 is the number of classes\n","\n","stacked = torch.stack(\n","    (\n","        test_labels\n","        ,test_preds.argmax(dim=1)\n","    )\n","    ,dim=1\n",")\n","\n","for p in stacked:\n","    tl, pl = p.tolist()\n","    tl = int(tl)\n","    pl = int(pl)\n","    cmt[tl, pl] = cmt[tl, pl] + 1\n","\n","#Plot CM\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import confusion_matrix"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"60mDU9MHfJQS"},"source":["## TO CALCULATE THE CONFUSION MATRIX, USE THE CONFUSION_MATRIX FUNCTION\n","## PLOT THE MATRIX USING THE plot_confusion_matrix() FUNCTION DEFINED IN \"Defining required functions\" cell above."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-R4Lbl8GfG4O"},"source":["# Solution"]},{"cell_type":"code","metadata":{"id":"lPs7HX3xe_vD"},"source":["cm = confusion_matrix(test_labels.cpu(), test_preds.argmax(dim=1).cpu())\n","print(cm)\n","\n","plt.figure(figsize=(12, 12))\n","plot_confusion_matrix(cm, test_dataset.classes, True, 'Confusion matrix', cmap=plt.cm.Blues)\n","plt.savefig(save_model_to + 'confusionMatrix.eps', format='eps', bbox_inches='tight')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vuQgrxQufzPl"},"source":["# 9. Exercise VIII: Generating Explanations With LIME\n","\n","In this exercise you are required to use LIME in order to generate explanations for the classification of the image located under '/content/dataset/tomato-dataset-final/test/Tomato___Late_blight/image (1076)_cropped_1.JPG'."]},{"cell_type":"code","metadata":{"cellView":"form","id":"diOOEbb8hlmx"},"source":["from lime import lime_image\n","from skimage import io\n","from skimage import img_as_ubyte\n","from skimage.segmentation import mark_boundaries\n","\n","\n","#@title Define Prerequisite Functions\n","Pretrainedmodel =  model\n","def get_PCNN_image(path):\n","  image = cv2.imread(path)\n","  image = cv2.resize(image, (226,226))\n","  return image\n","\n","PerturbationImgs = []\n","\n","def batch_predictPDCNN(images):\n","    Pretrainedmodel.eval()\n","    batch = torch.stack(tuple(preprocess_transform(i) for i in images), dim=0)\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    Pretrainedmodel.to(device)\n","    batch = batch.to(device)\n","    logits = Pretrainedmodel(batch)\n","    probs = F.softmax(logits, dim=1)\n","\n","    for image in images:\n","      PerturbationImgs.append(image)\n","\n","    return probs.detach().cpu().numpy()\n","\n","def get_preprocess_transform():\n","    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                                    std=[0.229, 0.224, 0.225])     \n","    transf = transforms.Compose([\n","        transforms.ToTensor(),\n","        normalize\n","    ])    \n","\n","    return transf    \n","\n","preprocess_transform = get_preprocess_transform()\n","\n","explainerPCNN = lime_image.LimeImageExplainer()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VjDpl_0Mk6xF"},"source":["## USE LIME TO GENERATE SUPERPIXELS ##\n","## THEN GENERATE PERTURBATIONS ##\n","## FINALLY, GENERATE THE EXPLANATION FOR image (1076) of the tomato_late_blight test set."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KCZ7IVa5h2a1"},"source":["# Solution"]},{"cell_type":"code","metadata":{"id":"h9OsnU9Zh7a5"},"source":["image2explain = '/content/dataset/tomato-dataset-final/test/Tomato___Late_blight/image (1076)_cropped_1.JPG'\n","PCNNimg = get_PCNN_image(image2explain)\n","PCNNimg = cv2.cvtColor(PCNNimg, cv2.COLOR_BGR2RGB)\n","explanationPCNN = explainerPCNN.explain_instance(PCNNimg, \n","                                         batch_predictPDCNN, top_labels=5, hide_color=0, num_samples=5000)\n","\n","tempPCNN, maskPCNN = explanationPCNN.get_image_and_mask(explanationPCNN.top_labels[0], positive_only=True, num_features=1, hide_rest=False)\n","\n","###############################\n","## SUPER PIXEL PERTURPATIONS ##\n","###############################\n","from matplotlib import gridspec\n","## VISUALIZE SOME PERTURBATIONS\n","# create a figure\n","fig = plt.figure()\n","# to change size of subplot's\n","fig.set_figheight(5)\n","# set width of each subplot as 8\n","fig.set_figwidth(15)\n","\n","# create grid for different subplots\n","spec = gridspec.GridSpec(ncols=5, nrows=2, wspace=0.1, hspace=0.1)\n","\n","print(\"PERTURBATIONS:\")\n","i=0\n","for perturbationImg in PerturbationImgs:\n","    p = fig.add_subplot(spec[i])\n","    p.axis('off')\n","    p.imshow(perturbationImg)\n","    i = i + 1\n","    if i > 9:\n","      break\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4OjoVN79kpFG"},"source":["#######################\n","## SHOW  EXPLANATION ##\n","#######################\n","print(\"FINAL EXPLANATION:\")\n","tempCNNP, maskCNNP = explanationPCNN.get_image_and_mask(explanationPCNN.top_labels[0], positive_only=False, num_features=1, hide_rest=False)\n","fig, (ax1) = plt.subplots(1, 1, figsize=(5,5))\n","ax1.bbox_inches='tight'\n","ax1.pad_inches = 0\n","ax1.axis('off')\n","plt.subplots_adjust(wspace=0, hspace=0)\n","plt.imshow(mark_boundaries(tempCNNP, maskCNNP))"],"execution_count":null,"outputs":[]}]}