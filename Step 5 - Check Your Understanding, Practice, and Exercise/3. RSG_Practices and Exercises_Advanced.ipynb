{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"RSG_Exercises_Apple.ipynb","provenance":[],"collapsed_sections":["EW_RbYrrRXOo","Wp2Ej74byTL-","c4lR3qZZluTM","FqoG0D16QxhL","iA3K-MgHdvIk","-R4Lbl8GfG4O","KCZ7IVa5h2a1"],"toc_visible":true,"authorship_tag":"ABX9TyNrtRMuwXfyqRjuk40F4ZGj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"1diqC4dBz1xc"},"source":["# Ready, Steady, Go AI (*Advanced Exercises*)"]},{"cell_type":"markdown","metadata":{"id":"MlQSL0no0uYT"},"source":["This notebook is a supplement to the paper, **Ready, Steady, Go AI: A Practical Tutorial on Explainable Artificial Intelligence and Its Applications in Phenomics Image Analysis** (submitted to *Patterns, 2021*) by Farid Nakhle and Antoine Harfouche\n","\n","Read the accompanying paper [here](https://doi.org).\n","\n","If you have any questions or feedback regarding this tutorial, please contact Farid Nakhle <<farid.nakhle@gmail.com>> or Antoine Harfouche <<aharfouche@unitus.it>>."]},{"cell_type":"markdown","metadata":{"id":"X5XoL1aD07Qo"},"source":["# Table of Contents\n"]},{"cell_type":"markdown","metadata":{"id":"SV7BfDp03T2j"},"source":["* **1. Introduction**\n","* **2. Exercise I: Splitting Data**\n","* **3. Exercise II: Cropping Apple Leaf Images Using a YOLO Model Pretrained on Tomato Leaves**\n","* **4. Exercise III: Segmenting Apple Leaf Images Using a SegNet Model Pretrained on Tomato Leaves**\n","* **5. Exercise IV: Descriptive Data Analysis**\n","* **6. Exercise V: Balancing the Dataset**\n","* **7. Exercise VI: Training and Classifying Using the DenseNet-161 Pretrained DCNN algorithm**\n","* **8. Exercise VII: Generating Confusion Matrix**\n","* **9. Exercise VIII: Generating Explanations With LIME (Quickshift and Compact-Watershed)**"]},{"cell_type":"markdown","metadata":{"id":"7btTy9923b9k"},"source":["# 1. Introduction\n"]},{"cell_type":"markdown","metadata":{"id":"iVEjYGUW3iMI"},"source":["Before attempting to resolve the exercises found in this notebook, visit our Github repository and try to open and run all the notebooks provided by the tutorial. \n","\n","Here, the solution for each exercise can be found in a hidden code cell at its end.\n","\n","Users should try to solve the exercises with the help of the notebooks provided by the tutorial before looking at the solution."]},{"cell_type":"markdown","metadata":{"id":"OnMTvK_SBSro"},"source":["As a reminder, we are working with the PlantVillage dataset, originally obtained from [here](http://dx.doi.org/10.17632/tywbtsjrjv.1).\n","For the following exercises, we will be working with a subset of PlantVillage containing the apple classes only. We have made the subset available [here](http://faridnakhle.com/pv/PlantVillage_Apple.zip). \n","\n","**It is important to note that Colab deletes all unsaved data once the instance is recycled. Therefore, remember to download your results once you run the code.**"]},{"cell_type":"markdown","metadata":{"id":"5n-qgOHeBPuO"},"source":["#2. Exercise I: Data Splitting\n"]},{"cell_type":"markdown","metadata":{"id":"hmP2F-VcR8RX"},"source":["**A.** Write a code that uses HTTP requests to download the PlantVillage apple leaves dataset available on the link provided in the introduction. The dataset must be saved then extracted to /content/dataset/original/.\n","\n","**B.** Use pip to install split-folders, then write a commant to split the dataset into training, validation, and testing sets. Use the following split ratio: training: 60%, validation: 20%, testing: 20%. The split dataset must be saved under /content/dataset/split/"]},{"cell_type":"code","metadata":{"id":"LQid3v_LSdi8"},"source":["import requests\n","import os\n","import zipfile\n","\n","############################\n","### Start Your Code Here ###\n","############################\n","\n","\n","\n","## Hint 1: ##\n","## Google Colab allows you to use the symbol \"!\" to execute commands from the underlying operating system.\n","## Example: To use pip, you can write !pip install\n","\n","## Hint 2: ##\n","## The split-folders parameters and usage instructions can be found here: https://pypi.org/project/split-folders/ ##"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EW_RbYrrRXOo"},"source":["# Solution"]},{"cell_type":"code","metadata":{"id":"2QkBhoIt66D-"},"source":["!rm -R /content/dataset/original/\n","!rm -R /content/dataset/split/\n","\n","import requests\n","import os\n","import zipfile\n","\n","dataset_url = \"http://faridnakhle.com/pv/PlantVillage_Apple.zip\"\n","save_data_to = \"/content/dataset/original/\"\n","dataset_file_name = \"dataset.zip\"\n","\n","if not os.path.exists(save_data_to):\n","    os.makedirs(save_data_to)\n","\n","r = requests.get(dataset_url, stream = True, headers={\"User-Agent\": \"Ready, Steady, Go AI\"})\n","\n","print(\"Downloading dataset...\")  \n","\n","with open(save_data_to + dataset_file_name, \"wb\") as file: \n","    for block in r.iter_content(chunk_size = 1024):\n","         if block: \n","             file.write(block)\n","\n","## Extract downloaded zip dataset file\n","print(\"Dataset downloaded\")  \n","print(\"Extracting files...\")  \n","with zipfile.ZipFile(save_data_to + dataset_file_name, 'r') as zip_dataset:\n","    zip_dataset.extractall(save_data_to)\n","\n","## Delete the zip file as we no longer need it\n","os.remove(save_data_to + dataset_file_name)\n","print(\"All done!\")  \n","\n","## SPLIT \n","!pip install split-folders tqdm\n","!splitfolders --output \"/content/dataset/split/\" --seed 1337 --ratio .8 .1 .1 -- \"/content/dataset/original\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r67fWPR_yLKl"},"source":["# 3. Exercise II: Cropping Apple Leaf Images Using a YOLO Model Pretrained on Tomato Leaves\n","\n","Before you start, make sure to run the \"Install and import prequisites\", \"Download pretrained model\", and \"Define prerequisite functions\" code cells.\n","\n","This exercise uses the YOLO model previously trained on cropping tomato leaf images to crop the apple leaf images.\n","\n","You are required to write the cropping function and to modify the code for it to save the cropped images under /content/dataset/cropped/\n","\n","After that, you should be able to run the \"Preview a Cropped Image\" code cell and see a sample image from the results."]},{"cell_type":"code","metadata":{"id":"isSivclMykq1","cellView":"form"},"source":["#@title Install and import prerequisites\n","!git clone https://github.com/ultralytics/yolov3\n","%cd yolov3\n","%pip install -qr requirements.txt \n","import torch\n","from IPython.display import Image\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GBK4Ma_ZyM1c","cellView":"form"},"source":["#@title Download pretrained model\n","\n","model_URL = \"http://faridnakhle.com/pv/models/YOLOv3.zip\"\n","save_data_to = \"/content/models/\"\n","model_file_name = \"yolo.zip\"\n","\n","if not os.path.exists(save_data_to):\n","    os.makedirs(save_data_to)\n","\n","print(\"Downloading model...\")  \n","\n","r = requests.get(model_URL, stream = True, headers={\"User-Agent\": \"Ready, Steady, Go AI\"})\n","with open(save_data_to + model_file_name, \"wb\") as file: \n","    for block in r.iter_content(chunk_size = 1024):\n","         if block: \n","             file.write(block)\n","\n","## Extract downloaded zip dataset file\n","print(\"Model downloaded\")  \n","print(\"Extracting files...\")\n","\n","with zipfile.ZipFile(save_data_to + model_file_name, 'r') as zip_dataset:\n","    zip_dataset.extractall(save_data_to)\n","print(\"All done!\")  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"QvxAWt0cVntb"},"source":["#@title Define prerequisite functions\n","import os\n","import cv2\n","import random\n","import numpy as np\n","\n","\n","def plot_grid(img, line_color=(0, 255, 0), thickness=1, type_=cv2.LINE_AA, pxstep=20, pystep=20):\n","    x = pystep\n","    y = pxstep\n","\n","    while x < img.shape[1]:\n","        cv2.line(img, (x, 0), (x, img.shape[0]), color=line_color, lineType=type_, thickness=thickness)\n","        x += pystep\n","\n","    while y < img.shape[0]:\n","        cv2.line(img, (0, y), (img.shape[1], y), color=line_color, lineType=type_, thickness=thickness)\n","        y += pxstep\n","def plot_borders(img, line_color=(0, 255, 0), thickness=1):\n","    cv2.rectangle(img,(0 ,0),(img.shape[1]-thickness,img.shape[0]-thickness), line_color, thickness)\n","\n","def myround(x, base=5):\n","    return base * round(x/base)\n","def plot_overlay(x, img, color, alpha,\n"," pxstep=20, pystep=20):\n","    overlay = img.copy()\n","    x0, x1, x2, x3 = int(x[0]), int(x[1]), int(x[2]), int(x[3])\n","\n","    x0 = myround(x0,pystep)\n","    x1 = myround(x1,pxstep)\n","    x2 = myround(x2,pystep)\n","    x3 = myround(x3,pxstep)\n","\n","    c1, c2 = (x0, x1), (x2, x3)\n","    cv2.rectangle(overlay, c1, c2, color, -1)\n","    # apply the overlay\n","    cv2.addWeighted(overlay, alpha, img, 1 - alpha, 0, img)\n","\n","\n","\n","\n","!cd /content/yolov3/\n","import argparse\n","import time\n","from pathlib import Path\n","\n","import cv2\n","import torch\n","import torch.backends.cudnn as cudnn\n","from numpy import random\n","\n","from models.experimental import attempt_load\n","from utils.datasets import LoadStreams, LoadImages\n","from utils.general import check_img_size, non_max_suppression, apply_classifier, scale_coords, xyxy2xywh, \\\n","    strip_optimizer, set_logging, increment_path\n","from utils.plots import plot_one_box\n","from utils.torch_utils import select_device, load_classifier, time_synchronized\n","\n","import glob\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PzchcqzhWUJM"},"source":["In the next code cell, you need to modify the crop_object function so it can crop images based on the detected bounding box by YOLO.\n","\n","Next, in the crop function, you must modify the code so that the cropped images are saved in /content/dataset/cropped/"]},{"cell_type":"code","metadata":{"id":"QLqPXyPq4dG0"},"source":["def crop_object(img, coords, img_path):\n","    # get box coords\n","    xmin = int(coords[0])\n","    ymin = int(coords[1])\n","    xmax = int(coords[2])\n","    ymax = int(coords[3])\n","    # WRITE A CODE HERE TO CROP THE IMAGE. \n","    # HINT: THE IMAGE IS IN THE \"img\" VARIABLE\n","    \n","    #WRITE A CODE TO SAVE THE IMAGE USING cv2\n","    # SAVE THE IMAGE TO THE PATH STORED IN \"img_path\"\n","\n","\n","\"\"\"\n","YOU NEED TO MODIFY THIS FUNCTION IN ORDER TO SAVE THE CROPPED IMAGE\n","UNDER /content/dataset/cropped/\n","\"\"\"\n","def crop(dataset_dir='', model_path='/content/yolov3/runs/train/exp/best.pt'):\n","    save_txt, imgsz = False, 224\n","    weights = model_path\n","    projectP = 'runs/detect'\n","    projectNameP = 'exp'\n","    save_img = True\n","    view_img = True\n","\n","    save_dir = Path(increment_path(Path(projectP) / projectNameP, False))  # increment run\n","    (save_dir / 'labels' if save_txt else save_dir).mkdir(parents=True, exist_ok=True)  # make dir\n","\n","    #loop over train, val, and test set\n","    trainTestVarDirs = glob.glob(dataset_dir + \"*\")\n","    for setDir in trainTestVarDirs:\n","      splitDir = os.path.basename(setDir)\n","      setClasses = glob.glob(setDir + \"/*\")\n","      for setClass in setClasses:\n","        # Directories\n","        classDir = os.path.basename(setClass)\n","        finalSaveDir = os.path.join(save_dir, splitDir, classDir)\n","        Path(finalSaveDir).mkdir(parents=True, exist_ok=True)\n","        source = setClass\n","\n","        # Initialize\n","        set_logging()\n","        device = select_device('0')\n","        half = device.type != 'cpu'  # half precision only supported on CUDA\n","\n","        # Load model\n","        model = attempt_load(weights, map_location=device)  # load FP32 model\n","        imgsz = check_img_size(imgsz, s=model.stride.max())  # check img_size\n","        \n","        #introducing grid size\n","        gs = model.stride.max()\n","        #end\n","\n","        if half:\n","            model.half()  # to FP16\n","\n","        # Second-stage classifier\n","        classify = False\n","        if classify:\n","            modelc = load_classifier(name='resnet101', n=2)  # initialize\n","            modelc.load_state_dict(torch.load('weights/resnet101.pt', map_location=device)['model']).to(device).eval()\n","\n","        # Set Dataloader\n","        vid_path, vid_writer = None, None\n","        \n","        dataset = LoadImages(source, img_size=imgsz)\n","\n","        # Get names and colors\n","        names = model.module.names if hasattr(model, 'module') else model.names\n","        colors = [[random.randint(0, 255) for _ in range(3)] for _ in names]\n","\n","        colors = [[217, 175, 78]]\n","\n","        # Run inference\n","        t0 = time.time()\n","        img = torch.zeros((1, 3, imgsz, imgsz), device=device)  # init img\n","        _ = model(img.half() if half else img) if device.type != 'cpu' else None  # run once\n","        for path, img, im0s, vid_cap in dataset:\n","            img = torch.from_numpy(img).to(device)\n","            img = img.half() if half else img.float()  # uint8 to fp16/32\n","            img /= 255.0  # 0 - 255 to 0.0 - 1.0\n","            if img.ndimension() == 3:\n","                img = img.unsqueeze(0)\n","\n","            # Inference\n","            t1 = time_synchronized()\n","            pred = model(img, augment=True)[0]\n","\n","            # Apply NMS\n","            final_pred = non_max_suppression(pred, 0.15, 0.3, classes=0, agnostic=True)\n","            pred = non_max_suppression(pred, 0.00005, 1, classes=0, agnostic=True)\n","            t2 = time_synchronized()\n","\n","            # Apply Classifier\n","            if classify:\n","                pred = apply_classifier(pred, modelc, img, im0s)\n","\n","            # Process detections\n","            for i, det in enumerate(pred):  # detections per image\n","                \n","                p, s, im0 = Path(path), '', im0s.copy()\n","\n","                imoriginal = im0.copy()\n","                #plot grid\n","                numofsquares = int(imgsz/int(gs))\n","                rowstep = int(im0.shape[0]/numofsquares)\n","                colstep = int(im0.shape[1]/numofsquares)\n","                plot_borders(im0, line_color=(0,0,0), thickness=2)\n","                gridim_solo = im0.copy()\n","                plot_grid(gridim_solo, pxstep=rowstep, pystep=colstep, line_color=(0,0,0), thickness=2)\n","                #end plot grid\n","     \n","                save_path = str(finalSaveDir + \"/\" + p.name)\n","                s += '%gx%g ' % img.shape[2:]  # print string\n","                gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]  # normalization gain whwh\n","                if len(det):\n","                    # Rescale boxes from img_size to im0 size\n","                    det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0.shape).round()\n","\n","                    # Print results\n","                    for c in det[:, -1].unique():\n","                        n = (det[:, -1] == c).sum()  # detections per class\n","                        s += '%g %ss, ' % (n, names[int(c)])  # add to string\n","                    \n","                    # Write results\n","                    for *xyxy, conf, cls in reversed(det):\n","                        if save_img or view_img:  # Add bbox to image\n","                            label = ''#'%s %.2f' % (names[int(cls)], conf)\n","                            plot_one_box(xyxy, im0, label=label, color=colors[int(cls)], line_thickness=1)\n","                            \n","                \n","\n","                # Print time (inference + NMS)\n","                print('%sDone. (%.3fs)' % (s, t2 - t1))\n","\n","                # Save results (image with detections)\n","                if save_img:\n","                    cv2.imwrite(save_path + \"_original.jpg\", imoriginal)\n","                    cv2.imwrite(save_path, im0)\n","                    cv2.imwrite(save_path + \"_grid.jpg\", gridim_solo)\n","                        \n","\n","\n","            # SAVE FINAL CROPPED IMAGES\n","            # Process detections\n","            for i, det in enumerate(final_pred):  # detections per image\n","                \n","                p, s, im0 = Path(path), '', im0s\n","                im2 = im0.copy() #to use with grid/map\n","                #background\n","                numofsquares = int(imgsz/int(gs))\n","                rowstep = int(im0.shape[0]/numofsquares)\n","                colstep = int(im0.shape[1]/numofsquares)\n","                plot_overlay([0,0, im2.shape[1], im2.shape[0]], im2, color=(255, 255, 255), alpha=0.7, pxstep=rowstep, pystep=colstep)\n","                \n","                #borders\n","                plot_borders(im2, line_color=(0,0,0), thickness=2)\n","                plot_borders(im0, line_color=(0,0,0), thickness=2)\n","\n","                save_path = str(finalSaveDir + \"/\" +  p.name)\n","                s += '%gx%g ' % img.shape[2:]  # print string\n","                gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]  # normalization gain whwh\n","                if len(det):\n","                    # Rescale boxes from img_size to im0 size\n","                    det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0.shape).round()\n","\n","                    # Print results\n","                    for c in det[:, -1].unique():\n","                        n = (det[:, -1] == c).sum()  # detections per class\n","                        s += '%g %ss, ' % (n, names[int(c)])  # add to string\n","\n","                    #FUNCTION custom crop\n","                    CROP = True\n","                    if CROP:\n","                        fidx = 0\n","                        for *xyxy, conf, cls in reversed(det):\n","                            if save_img or view_img:\n","                                fidx = fidx + 1\n","                                crop_object(im0, xyxy, str(finalSaveDir + \"/\" +  (p.stem + \"_cropped_\" + str(fidx) + p.suffix)))\n","                    #END\n","                    \n","                    # Write results\n","                    for *xyxy, conf, cls in reversed(det):\n","                        if save_img or view_img:  # Add bbox to image\n","                            label = ''#'%s %.2f' % (names[int(cls)], conf)\n","                            plot_one_box(xyxy, im0, label=label, color=colors[int(cls)], line_thickness=2)\n","                            plot_overlay(xyxy, im2, color=colors[int(cls)], alpha=0.7, pxstep=rowstep, pystep=colstep)\n","                else:\n","                    cv2.imwrite(save_path + \"_not_cropped.jpg\", im0)\n","\n","\n","                gridim = im2.copy()\n","                plot_grid(gridim, pxstep=rowstep, pystep=colstep, line_color=(0,0,0), thickness=2)\n","                \n","                # Print time (inference + NMS)\n","                print('%sDone. (%.3fs)' % (s, t2 - t1))\n","\n","                # Save results (image with detections)\n","                if save_img:\n","                    cv2.imwrite(save_path + \"_map.jpg\", gridim)\n","                    cv2.imwrite(save_path + \"_final.jpg\", im0)\n","\n","        if save_txt or save_img:\n","            s = f\"\\n{len(list(finalSaveDir.glob('labels/*.txt')))} labels saved to {finalSaveDir + '/' + 'labels'}\" if save_txt else ''\n","            print(f\"Results saved to {finalSaveDir}{s}\")\n","\n","        print('Done. (%.3fs)' % (time.time() - t0))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WXcbhb9y3yag"},"source":["# REPLACE ? with your answer\n","crop(dataset_dir=?, model_path='/content/models/weights/RSGAI_YOLOv3.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Wp2Ej74byTL-"},"source":["# Solution"]},{"cell_type":"code","metadata":{"id":"K1-hL3igVJ5d"},"source":["import os\n","import cv2\n","import random\n","import numpy as np\n","\n","def crop_object(img, coords, img_path):\n","    # get box coords\n","    xmin = int(coords[0])\n","    ymin = int(coords[1])\n","    xmax = int(coords[2])\n","    ymax = int(coords[3])\n","    # crop detection from image\n","    cropped_img = img[ymin:ymax, xmin:xmax]\n","    # save image\n","    cv2.imwrite(img_path, cropped_img)\n","\n","def crop(dataset_dir='', model_path='/content/yolov3/runs/train/exp/best.pt'):\n","    save_txt, imgsz = False, 224\n","    weights = model_path\n","    projectP = 'runs/detect'\n","    projectNameP = 'exp'\n","    save_img = True\n","    view_img = True\n","\n","    save_dir = Path(increment_path(Path(projectP) / projectNameP, False))  # increment run\n","    (save_dir / 'labels' if save_txt else save_dir).mkdir(parents=True, exist_ok=True)  # make dir\n","    cropped_save_dir = Path('/content/dataset/cropped')\n","    cropped_save_dir.mkdir(parents=True, exist_ok=True)  # make dir\n","\n","    #loop over train, val, and test set\n","    trainTestVarDirs = glob.glob(dataset_dir + \"*\")\n","    for setDir in trainTestVarDirs:\n","      splitDir = os.path.basename(setDir)\n","      setClasses = glob.glob(setDir + \"/*\")\n","      for setClass in setClasses:\n","        # Directories\n","        classDir = os.path.basename(setClass)\n","        finalSaveDir = os.path.join(save_dir, splitDir, classDir)\n","        Path(finalSaveDir).mkdir(parents=True, exist_ok=True)\n","        croppedSaveDir = os.path.join(cropped_save_dir, splitDir, classDir)\n","        Path(croppedSaveDir).mkdir(parents=True, exist_ok=True)\n","        source = setClass\n","        \n","\n","        # Initialize\n","        set_logging()\n","        device = select_device('0')\n","        half = device.type != 'cpu'  # half precision only supported on CUDA\n","\n","        # Load model\n","        model = attempt_load(weights, map_location=device)  # load FP32 model\n","        imgsz = check_img_size(imgsz, s=model.stride.max())  # check img_size\n","        \n","        #introducing grid size\n","        gs = model.stride.max()\n","        #end\n","\n","        if half:\n","            model.half()  # to FP16\n","\n","        # Second-stage classifier\n","        classify = False\n","        if classify:\n","            modelc = load_classifier(name='resnet101', n=2)  # initialize\n","            modelc.load_state_dict(torch.load('weights/resnet101.pt', map_location=device)['model']).to(device).eval()\n","\n","        # Set Dataloader\n","        vid_path, vid_writer = None, None\n","        \n","        dataset = LoadImages(source, img_size=imgsz)\n","\n","        # Get names and colors\n","        names = model.module.names if hasattr(model, 'module') else model.names\n","        colors = [[random.randint(0, 255) for _ in range(3)] for _ in names]\n","\n","        colors = [[217, 175, 78]]\n","\n","        # Run inference\n","        t0 = time.time()\n","        img = torch.zeros((1, 3, imgsz, imgsz), device=device)  # init img\n","        _ = model(img.half() if half else img) if device.type != 'cpu' else None  # run once\n","        for path, img, im0s, vid_cap in dataset:\n","            img = torch.from_numpy(img).to(device)\n","            img = img.half() if half else img.float()  # uint8 to fp16/32\n","            img /= 255.0  # 0 - 255 to 0.0 - 1.0\n","            if img.ndimension() == 3:\n","                img = img.unsqueeze(0)\n","\n","            # Inference\n","            t1 = time_synchronized()\n","            pred = model(img, augment=True)[0]\n","\n","            # Apply NMS\n","            final_pred = non_max_suppression(pred, 0.15, 0.3, classes=0, agnostic=True)\n","            pred = non_max_suppression(pred, 0.00005, 1, classes=0, agnostic=True)\n","            t2 = time_synchronized()\n","\n","            # Apply Classifier\n","            if classify:\n","                pred = apply_classifier(pred, modelc, img, im0s)\n","\n","            # Process detections\n","            for i, det in enumerate(pred):  # detections per image\n","                \n","                p, s, im0 = Path(path), '', im0s.copy()\n","\n","                imoriginal = im0.copy()\n","                #plot grid\n","                numofsquares = int(imgsz/int(gs))\n","                rowstep = int(im0.shape[0]/numofsquares)\n","                colstep = int(im0.shape[1]/numofsquares)\n","                plot_borders(im0, line_color=(0,0,0), thickness=2)\n","                gridim_solo = im0.copy()\n","                plot_grid(gridim_solo, pxstep=rowstep, pystep=colstep, line_color=(0,0,0), thickness=2)\n","                #end plot grid\n","     \n","                save_path = str(finalSaveDir + \"/\" + p.name)\n","                s += '%gx%g ' % img.shape[2:]  # print string\n","                gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]  # normalization gain whwh\n","                if len(det):\n","                    # Rescale boxes from img_size to im0 size\n","                    det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0.shape).round()\n","\n","                    # Print results\n","                    for c in det[:, -1].unique():\n","                        n = (det[:, -1] == c).sum()  # detections per class\n","                        s += '%g %ss, ' % (n, names[int(c)])  # add to string\n","                    \n","                    # Write results\n","                    for *xyxy, conf, cls in reversed(det):\n","                        if save_img or view_img:  # Add bbox to image\n","                            label = ''#'%s %.2f' % (names[int(cls)], conf)\n","                            plot_one_box(xyxy, im0, label=label, color=colors[int(cls)], line_thickness=1)\n","                            \n","                \n","\n","                # Print time (inference + NMS)\n","                print('%sDone. (%.3fs)' % (s, t2 - t1))\n","\n","                # Save results (image with detections)\n","                if save_img:\n","                    cv2.imwrite(save_path + \"_original.jpg\", imoriginal)\n","                    cv2.imwrite(save_path, im0)\n","                    cv2.imwrite(save_path + \"_grid.jpg\", gridim_solo)\n","                        \n","\n","\n","            # SAVE FINAL CROPPED IMAGES\n","            # Process detections\n","            for i, det in enumerate(final_pred):  # detections per image\n","                \n","                p, s, im0 = Path(path), '', im0s\n","                im2 = im0.copy() #to use with grid/map\n","                #background\n","                numofsquares = int(imgsz/int(gs))\n","                rowstep = int(im0.shape[0]/numofsquares)\n","                colstep = int(im0.shape[1]/numofsquares)\n","                plot_overlay([0,0, im2.shape[1], im2.shape[0]], im2, color=(255, 255, 255), alpha=0.7, pxstep=rowstep, pystep=colstep)\n","                \n","                #borders\n","                plot_borders(im2, line_color=(0,0,0), thickness=2)\n","                plot_borders(im0, line_color=(0,0,0), thickness=2)\n","\n","                save_path = str(finalSaveDir + \"/\" +  p.name)\n","                s += '%gx%g ' % img.shape[2:]  # print string\n","                gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]  # normalization gain whwh\n","                if len(det):\n","                    # Rescale boxes from img_size to im0 size\n","                    det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0.shape).round()\n","\n","                    # Print results\n","                    for c in det[:, -1].unique():\n","                        n = (det[:, -1] == c).sum()  # detections per class\n","                        s += '%g %ss, ' % (n, names[int(c)])  # add to string\n","\n","                    #FUNCTION custom crop\n","                    CROP = True\n","                    if CROP:\n","                        fidx = 0\n","                        for *xyxy, conf, cls in reversed(det):\n","                            if save_img or view_img:\n","                                fidx = fidx + 1\n","                                crop_object(im0, xyxy, str(finalSaveDir + \"/\" +  (p.stem + \"_cropped_\" + str(fidx) + p.suffix)))\n","                                crop_object(im0, xyxy, str(croppedSaveDir + \"/\" +  (p.stem + \"_cropped_\" + str(fidx) + p.suffix)))\n","                    #END\n","                    \n","                    # Write results\n","                    for *xyxy, conf, cls in reversed(det):\n","                        if save_img or view_img:  # Add bbox to image\n","                            label = ''#'%s %.2f' % (names[int(cls)], conf)\n","                            plot_one_box(xyxy, im0, label=label, color=colors[int(cls)], line_thickness=2)\n","                            plot_overlay(xyxy, im2, color=colors[int(cls)], alpha=0.7, pxstep=rowstep, pystep=colstep)\n","                else:\n","                    cv2.imwrite(save_path + \"_not_cropped.jpg\", im0)\n","\n","\n","                gridim = im2.copy()\n","                plot_grid(gridim, pxstep=rowstep, pystep=colstep, line_color=(0,0,0), thickness=2)\n","                \n","                # Print time (inference + NMS)\n","                print('%sDone. (%.3fs)' % (s, t2 - t1))\n","\n","                # Save results (image with detections)\n","                if save_img:\n","                    cv2.imwrite(save_path + \"_map.jpg\", gridim)\n","                    cv2.imwrite(save_path + \"_final.jpg\", im0)\n","\n","        if save_txt or save_img:\n","            s = f\"\\n{len(list(finalSaveDir.glob('labels/*.txt')))} labels saved to {finalSaveDir + '/' + 'labels'}\" if save_txt else ''\n","            print(f\"Results saved to {finalSaveDir}{s}\")\n","\n","        print('Done. (%.3fs)' % (time.time() - t0))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JDxe16VQyV7-"},"source":["crop(dataset_dir='/content/dataset/split/', model_path='/content/models/weights/RSGAI_YOLOv3.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cqOjSM_Y-OEU"},"source":["# Preview a Cropped Image"]},{"cell_type":"code","metadata":{"id":"XFm6eyhw-GJJ","cellView":"form"},"source":["#@title Generate preview\n","\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","import matplotlib.image as mpimg\n","import os\n","import glob\n","lastExp = max(glob.glob(os.path.join('/content/yolov3/runs/detect','*/' )), key=os.path.getmtime)\n","\n","imgPath = lastExp + 'test/Apple___Black_rot/image (139).JPG'\n","oringinalImg = mpimg.imread(imgPath + \"_original.jpg\")\n","boundingBoxesImg = mpimg.imread(imgPath)\n","croppedImg = mpimg.imread(imgPath.replace(\".JPG\", \"_cropped_1.JPG\"))\n","gridImg = mpimg.imread(imgPath+ \"_grid.jpg\")\n","mapImg = mpimg.imread(imgPath+ \"_map.jpg\")\n","finaldetectImg = mpimg.imread(imgPath+ \"_final.jpg\")\n","\n","print(\"Original Image:\")\n","plt.axis('off')\n","plt.imshow(oringinalImg)\n","plt.show()\n","\n","print(\"Grid:\")\n","plt.axis('off')\n","plt.imshow(gridImg)\n","plt.show()\n","\n","print(\"Bounding Boxes:\")\n","plt.axis('off')\n","plt.imshow(boundingBoxesImg)\n","plt.show()\n","\n","print(\"Probability Map:\")\n","plt.axis('off')\n","plt.imshow(mapImg)\n","plt.show()\n","\n","print(\"Final Detection:\")\n","plt.axis('off')\n","plt.imshow(finaldetectImg)\n","plt.show()\n","\n","\n","print(\"Cropped Image:\")\n","plt.axis('off')\n","plt.imshow(croppedImg)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gyXcveLiiIcq"},"source":["# 4. Exercise III: Segmenting Apple Leaf Images Using a SegNet Model Pretrained on Tomato Leaves\n","\n","Before you start, make sure to run the \"Install and import prequisites\", \"Download pretrained model\", and \"Define prerequisite functions\" code cells.\n","\n","Next,  you are required to:\n","\n","**A.** Loop over the train, test, and val folder under /content/dataset/cropped\n","\n","**B.** Loop over different classes in each folder\n","\n","**C.** Loop over images\n","\n","**D.** Use the pretrained SegNet model to segment each image\n","\n","**E.** Save images under /content/dataset/segmented/ preserving the train, test, val directory  structure\n","\n","After that, you should be able to run the \"Preview a Segmented Image\" cell and see a sample image from the results."]},{"cell_type":"code","metadata":{"id":"LyU8pyFwi7Of","cellView":"form"},"source":["#@title Install and import prerequisites\n","!git clone https://github.com/divamgupta/image-segmentation-keras\n","%cd image-segmentation-keras\n","from keras_segmentation.models.segnet import segnet\n","print(\"Keras and SegNet are loaded\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IbJsLl84jNRP","cellView":"form"},"source":["#@title Download pretrained model\n","##########################\n","### DOWNLOAD THE MODEL ###\n","##########################\n","import requests\n","import os\n","import zipfile\n","## FEEL FREE TO CHANGE THESE PARAMETERS\n","model_URL = \"http://faridnakhle.com/pv/models/SegNet.zip\"\n","save_data_to = \"/content/models/\"\n","model_file_name = \"segnet.zip\"\n","#######################################\n","\n","if not os.path.exists(save_data_to):\n","    os.makedirs(save_data_to)\n","\n","print(\"Downloading model...\")  \n","\n","r = requests.get(model_URL, stream = True, headers={\"User-Agent\": \"Ready, Steady, Go AI\"})\n","with open(save_data_to + model_file_name, \"wb\") as file: \n","    for block in r.iter_content(chunk_size = 1024):\n","         if block: \n","             file.write(block)\n","\n","## Extract downloaded zip dataset file\n","print(\"Model downloaded\")  \n","print(\"Extracting files...\")\n","\n","with zipfile.ZipFile(save_data_to + model_file_name, 'r') as zip_dataset:\n","    zip_dataset.extractall(save_data_to)\n","print(\"All done!\")  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nk2PP9RWj1yr","cellView":"form"},"source":["#@title Define prerequisite functions\n","\n","import cv2\n","import numpy as np\n","from keras_segmentation.models.segnet import segnet\n","import glob\n","import os\n","from tqdm import tqdm\n","import six"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fAf_GX64Z3bs"},"source":["def segment(inptDir = \"\"):\n","\n","  modelName = \"/content/models/RSGAI_SegNet.hdf5\"\n","  model = segnet(n_classes=50 ,  input_height=320, input_width=640)\n","  model.load_weights(modelName)\n","\n","  outputDir = \"/content/dataset/segmented/\"\n","  inptDirGlob = glob.glob(inptDir + \"*\")\n","\n","  ########################\n","  #### YOUR CODE HERE ####\n","  ########################\n","  '''\n","    1- Loop over the train, test, and val folder under /content/dataset/cropped\n","    2- Loop over different classes in each folder\n","    3- Loop over images\n","    4- Use the SegNet model to segment each image\n","    5- Save images under /content/dataset/segmented/ preserving the train, test, val directory  structure\n","  '''\n","  #############################################\n","  print(\"Segmented images are saved in:\") \n","  print(outputDir)\n","segment(inptDir=\"/content/dataset/cropped/\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c4lR3qZZluTM"},"source":["# Solution"]},{"cell_type":"code","metadata":{"id":"w4qNAmFVls9-"},"source":["def segment(inptDir = \"\"):\n","\n","  modelName = \"/content/models/RSGAI_SegNet.hdf5\"\n","  model = segnet(n_classes=50 ,  input_height=320, input_width=640)\n","  model.load_weights(modelName)\n","\n","  outputDir = \"/content/dataset/segmented/\"\n","\n","  inptDirGlob = glob.glob(inptDir + \"*\")\n","  for setDir in inptDirGlob:\n","\n","    splitDir = os.path.basename(setDir)\n","    setClasses = glob.glob(setDir + \"/*\")\n","\n","    for setClass in setClasses:\n","\n","      classDir = os.path.basename(setClass)\n","      inptFolder = os.path.join(inptDir, splitDir, classDir)\n","      outputFolder = os.path.join(outputDir, splitDir, classDir)\n","\n","      if not os.path.exists(outputFolder):\n","          os.makedirs(outputFolder)\n","\n","      inps = glob.glob(os.path.join(inptFolder, \"*.jpg\")) + glob.glob(\n","          os.path.join(inptFolder, \"*.png\")) + \\\n","          glob.glob(os.path.join(inptFolder, \"*.jpeg\"))+ \\\n","          glob.glob(os.path.join(inptFolder, \"*.JPG\"))\n","      inps = sorted(inps)\n","\n","      if len(inps) > 0:\n","\n","        all_prs = []\n","\n","        for i, inp in enumerate(tqdm(inps)):\n","            if outputFolder is None:\n","                out_fname = None\n","            else:\n","                if isinstance(inp, six.string_types):\n","                    out_fname = os.path.join(outputFolder, os.path.basename(inp))\n","                else:\n","                    out_fname = os.path.join(outputFolder, str(i) + \".jpg\")\n","\n","            pr = model.predict_segmentation(\n","                inp=inp,\n","                out_fname=out_fname\n","            )\n","\n","            img = cv2.imread(inp)\n","            seg = cv2.imread(out_fname)\n","\n","            for row in range(0, len(seg)):\n","                for col in range(0, len(seg[0])):\n","                    #if np.all(seg[row, col] == [7,47,204]) == False:\n","                    #    img[row, col] = [0,0,0]\n","                    \n","                    if seg[row, col][0] > 50:\n","                        img[row, col] = [0,0,0]\n","            all_prs.append(pr)\n","            cv2.imwrite(out_fname, img)\n","\n","\n","  print(\"Segmented images are saved in:\") \n","  print(outputDir)\n","segment(inptDir=\"/content/dataset/cropped/\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q0gQLzqzmzeb"},"source":["# Preview a Segmented Image"]},{"cell_type":"code","metadata":{"id":"STmIaG74mWJN","cellView":"form"},"source":["#@title Generate Preview\n","import matplotlib.pyplot as plt\n","import matplotlib.image as mpimg\n","imgPath = '/content/dataset/cropped/test/Apple___Black_rot/image (139)_cropped_1.JPG'\n","segmemtedPath = '/content/dataset/segmented/test/Apple___Black_rot/image (139)_cropped_1.JPG'\n","\n","oringinalImg = mpimg.imread(imgPath)\n","segmentedImage = mpimg.imread(segmemtedPath)\n","\n","print(\"Original Image:\")\n","plt.axis('off')\n","plt.imshow(oringinalImg)\n","plt.show()\n","\n","print(\"Segmented Image:\")\n","plt.axis('off')\n","plt.imshow(segmentedImage)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i6BqFC8onFQM"},"source":["# 5. Exercise IV: Descriptive Data Analysis\n","\n","In this exercise, you are required to generate a bar plot of the data distribution over classes, showing the number of images per class. \n","\n","Hint: You need to loop over the classes in the segmented training data folder, then count the images in each. You can use matplotlib to generate the plot.\n","\n","Based on the above, you can decide whether or not there is a need for data balancing."]},{"cell_type":"code","metadata":{"id":"UmXZ2ag_n2DT"},"source":["### WRITE YOUR CODE HERE ###"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wgw_WVucp1l5"},"source":["# Solution"]},{"cell_type":"code","metadata":{"id":"8zD6vDw5oWiQ"},"source":["import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import os\n","import shutil\n","import cv2\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","train_dir = '/content/dataset/segmented/train/'\n","train_classes = [path for path in os.listdir(train_dir)]\n","train_imgs = dict([(ID, os.listdir(os.path.join(train_dir, ID))) for ID in train_classes])\n","train_classes_count = []\n","for trainClass in train_classes:\n","  train_classes_count.append(len(train_imgs[trainClass]))\n","\n","plt.figure(figsize=(15, 10))\n","g = sns.barplot(x=train_classes, y=train_classes_count)\n","g.set_xticklabels(labels=train_classes, rotation=30, ha='right')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HEHo-wEkL35G"},"source":["# 6. Exercise V: Balancing the Dataset\n","\n","Based on the results of the Descriptive Data Analysis (exercise IV), choose a corresponding data balancing technique to balance the dataset if needed.\n"," \n","**NB:** After data balancing, generate the data distribution plot again to analyze the new distribution of classes."]},{"cell_type":"code","metadata":{"id":"X3s2wtQXQVO3"},"source":["### YOUR CODE HERE ###"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FqoG0D16QxhL"},"source":["# Solution"]},{"cell_type":"markdown","metadata":{"id":"5itEDt_ow2Il"},"source":["It is clear that the dataset is unbalanced. \n","We can either downsample the healthy class and augment the cedar apple rust class to average image numbers to around 500 image per class, or we can augment all classes to be balanced.\n","\n","In our solution, we will augment all classes to 1500 images using Augmentor."]},{"cell_type":"code","metadata":{"id":"YgMgWADkPpDK"},"source":["!pip install Augmentor\n","import Augmentor\n","import os\n","\n","def makedir(path):\n","    '''\n","    if path does not exist in the file system, create it\n","    '''\n","    if not os.path.exists(path):\n","        os.makedirs(path)\n","\n","datasets_root_dir = '/content/dataset/segmented/'\n","dir = datasets_root_dir + 'train/'\n","target_dir = dir #same directory as input\n","makedir(target_dir)\n","\n","folders = [os.path.join(dir, folder) for folder in next(os.walk(dir))[1]]\n","target_folders = [os.path.join(target_dir, folder) for folder in next(os.walk(dir))[1]]\n","\n","requiredNbrOfImages = 1500\n","\n","for i in range(len(folders)):\n","    path, dirs, files = next(os.walk(folders[i]))\n","    nbrOfImages = len(files)\n","    nbrOfImagesNeeded = requiredNbrOfImages - nbrOfImages\n","      \n","    if nbrOfImagesNeeded > 0:\n","        tfd = target_folders[i]\n","        print (\"saving in \" + tfd)\n","        p = Augmentor.Pipeline(source_directory=folders[i], output_directory=tfd)\n","        p.rotate(probability=1, max_left_rotation=15, max_right_rotation=15)\n","        p.flip_left_right(probability=0.5)\n","        p.skew(probability=1, magnitude=0.2)\n","        p.flip_left_right(probability=0.5)\n","        p.shear(probability=1, max_shear_left=10, max_shear_right=10)\n","        p.flip_left_right(probability=0.5)\n","        p.sample(nbrOfImagesNeeded)\n","print(\"Dataset Augmented!\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FTVcB_sMaK9m"},"source":["# Display Final Data Distribution"]},{"cell_type":"code","metadata":{"id":"NGOMd_XhaKua","cellView":"form"},"source":["#@title Generate Data Distribution\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import os\n","import shutil\n","import cv2\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","train_dir = '/content/dataset/segmented/train/'\n","train_classes = [path for path in os.listdir(train_dir)]\n","train_imgs = dict([(ID, os.listdir(os.path.join(train_dir, ID))) for ID in train_classes])\n","train_classes_count = []\n","for trainClass in train_classes:\n","  train_classes_count.append(len(train_imgs[trainClass]))\n","\n","plt.figure(figsize=(15, 10))\n","g = sns.barplot(x=train_classes, y=train_classes_count)\n","g.set_xticklabels(labels=train_classes, rotation=30, ha='right')\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xcfN0hqtbeVy"},"source":["# 7. Exercise VI: Training and Classifying Using the DenseNet-161 Pretrained DCNN algorithm"]},{"cell_type":"markdown","metadata":{"id":"-NDTlsBgbxlC"},"source":["In this exercise, you are required to implement a DCNN algorithm that uses transfer learning to import a DenseNet-161 pretrained model.\n","You then need to train the algorithm on the segmented apple training set, and use the validation set to validate the model during training. \n","\n","Once training is complete, use the test set to test the overall accuraccy of the model."]},{"cell_type":"code","metadata":{"id":"yY1TnKkebxE4","cellView":"form"},"source":["#@title Global Variables Needed\n","\n","import argparse\n","import os\n","import time\n","\n","import matplotlib.pyplot as plt\n","\n","import torch\n","import numpy as np\n","from torch import nn\n","from torch import optim\n","import torch.nn.functional as F\n","from torchvision import datasets, transforms, models\n","from PIL import Image\n","from collections import OrderedDict\n","import json\n","\n","## YOU CAN CHANGE THESE VARIABLES    \n","EPOCHS = 100\n","BATCH_SIZE = 20\n","LEARNING_RATE = 0.0001\n","data_dir = '/content/dataset/segmented/'\n","save_checkpoints = True\n","save_model_to = '/content/output/'\n","!mkdir /content/output/\n","IMG_SIZE = 220\n","NUM_WORKERS = 1\n","using_gpu = torch.cuda.is_available()\n","print_every = 300\n","ARCH = 'densenet161'\n","######################################################"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nxh2mR9vbowR"},"source":["### YOUR CODE HERE###\n","\n","#HINT: Start by create three dataloaders: train_looader, val_loader, and test_loader\n","\n","''' STEPS:\n","- Build your model with corresponding layers\n","- Define loss and optimizer\n","- Create a function to measure accuracy\n","- Write the training loop\n","- Write a function to test with the testing set\n","'''"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iA3K-MgHdvIk"},"source":["# Solution"]},{"cell_type":"code","metadata":{"id":"iw4jLpf0dsaC"},"source":["def data_loader(root, batch_size=256, workers=1, pin_memory=True):\n","    traindir = os.path.join(root, 'train')\n","    valdir = os.path.join(root, 'val')\n","    testdir = os.path.join(root, 'test')\n","    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                                     std=[0.229, 0.224, 0.225])\n","\n","    train_dataset = datasets.ImageFolder(\n","        traindir,\n","        transforms.Compose([\n","            transforms.Resize(size=(IMG_SIZE, IMG_SIZE)),\n","            transforms.ToTensor(),\n","            normalize\n","        ])\n","    )\n","    val_dataset = datasets.ImageFolder(\n","        valdir,\n","        transforms.Compose([\n","            transforms.Resize(size=(IMG_SIZE, IMG_SIZE)),\n","            transforms.ToTensor(),\n","            normalize\n","        ])\n","    )\n","    test_dataset = datasets.ImageFolder(\n","        testdir,\n","        transforms.Compose([\n","            transforms.Resize(size=(IMG_SIZE, IMG_SIZE)),\n","            transforms.ToTensor(),\n","            normalize\n","        ])\n","    )\n","\n","    train_loader = torch.utils.data.DataLoader(\n","        train_dataset,\n","        batch_size=batch_size,\n","        shuffle=True,\n","        num_workers=workers,\n","        pin_memory=pin_memory,\n","        sampler=None\n","    )\n","    val_loader = torch.utils.data.DataLoader(\n","        val_dataset,\n","        batch_size=batch_size,\n","        shuffle=False,\n","        num_workers=workers,\n","        pin_memory=pin_memory\n","    )\n","    test_loader = torch.utils.data.DataLoader(\n","        test_dataset,\n","        batch_size=batch_size,\n","        shuffle=False,\n","        num_workers=workers,\n","        pin_memory=pin_memory\n","    )\n","    return train_loader, val_loader, test_loader, train_dataset, val_dataset, test_dataset\n","\n","# Data loading\n","train_loader, val_loader, test_loader, train_dataset, val_dataset, test_dataset = data_loader(data_dir, BATCH_SIZE, NUM_WORKERS, False)\n","print(\"Training Set: \" + str(len(train_loader.dataset)))\n","print(\"Validation Set: \" + str(len(val_loader.dataset)))\n","print(\"Testing Set: \" + str(len(test_loader.dataset)))\n","\n","# Freeze parameters so we don't backprop through them\n","hidden_layers = [10240, 1024]\n","def make_model(structure, hidden_layers, lr, preTrained):\n","    if structure==\"densenet161\":\n","        model = models.densenet161(pretrained=preTrained)\n","        input_size = 2208\n","    else:\n","        model = models.vgg16(pretrained=preTrained)\n","        input_size = 25088\n","    output_size = 102\n","    for param in model.parameters():\n","        param.requires_grad = False\n","\n","    classifier = nn.Sequential(OrderedDict([\n","                              ('dropout',nn.Dropout(0.5)),\n","                              ('fc1', nn.Linear(input_size, hidden_layers[0])),\n","                              ('relu1', nn.ReLU()),\n","                              ('fc2', nn.Linear(hidden_layers[0], hidden_layers[1])),\n","                              ('relu2', nn.ReLU()),\n","                              ('fc3', nn.Linear(hidden_layers[1], output_size)),\n","                              ('output', nn.LogSoftmax(dim=1))\n","                              ]))\n","\n","    model.classifier = classifier\n","    return model\n","\n","#############################\n","## TRANSFER LEARNING MODEL ##\n","#############################\n","## Setting the preTrained parameter to true in our make_model function\n","## will download a pretrained DenseNet-161 model from PyTorch software framework\n","## then we will train this model on our data, as previously done with the standard DCNN\n","model = make_model(ARCH, hidden_layers, LEARNING_RATE, True)\n","\n","\n","# define loss and optimizer\n","criterion = nn.NLLLoss()\n","optimizer = optim.Adam(model.classifier.parameters(), lr=LEARNING_RATE)\n","\n","def cal_accuracy(model, dataloader):\n","    validation_loss = 0\n","    accuracy = 0\n","    for i, (inputs,labels) in enumerate(dataloader):\n","                optimizer.zero_grad()\n","                inputs, labels = inputs.to('cuda') , labels.to('cuda')\n","                model.to('cuda')\n","                with torch.no_grad():    \n","                    outputs = model.forward(inputs)\n","                    validation_loss = criterion(outputs,labels)\n","                    ps = torch.exp(outputs).data\n","                    equality = (labels.data == ps.max(1)[1])\n","                    accuracy += equality.type_as(torch.FloatTensor()).mean()\n","                    \n","    validation_loss = validation_loss / len(dataloader)\n","    accuracy = accuracy /len(dataloader)\n","    \n","    return validation_loss, accuracy\n","\n","\n","def StartTraining(model, image_trainloader, image_valloader, epochs, print_every, criterion, optimizer, device='gpu'):\n","    epochs = epochs\n","    print_every = print_every\n","    steps = 0\n","\n","    # change to cuda\n","    model.to('cuda')\n","\n","    for e in range(epochs):\n","        running_loss = 0\n","        for ii, (inputs, labels) in enumerate(image_trainloader):\n","            steps += 1\n","\n","            inputs, labels = inputs.to('cuda'), labels.to('cuda')\n","\n","            optimizer.zero_grad()\n","\n","            # Forward and backward passes\n","            outputs = model.forward(inputs)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss += loss.item()\n","\n","            if steps % print_every == 0:\n","                model.eval()\n","                val_loss, train_ac = cal_accuracy(model, image_valloader)\n","                print(\"Epoch: {}/{}... | \".format(e+1, epochs),\n","                      \"Loss: {:.4f} | \".format(running_loss/print_every),\n","                      \"Validation Loss {:.4f} | \".format(val_loss),\n","                      \"Accuracy {:.4f}\".format(train_ac))\n","\n","                running_loss = 0\n","        \n","        if (save_checkpoints):\n","            model.class_to_idx = train_dataset.class_to_idx\n","            state = {\n","                        'structure' : ARCH,\n","                        'learning_rate': LEARNING_RATE,\n","                        'epochs': e,\n","                        'hidden_layers':hidden_layers,\n","                        'state_dict':model.state_dict(),\n","                        'class_to_idx':model.class_to_idx,\n","                        'optimizer_state_dict': optimizer.state_dict(),\n","                        'loss': loss\n","            }\n","            torch.save(state, save_model_to + ARCH + '_checkpoint_epoch_'+ str(e) +'_acc_'+str(train_ac)+'_.pth')\n","            torch.save(obj=model, f=save_model_to + ARCH + '_checkpoint_epoch_'+ str(e) +'_acc_'+str(train_ac)+'_MODEL.pth')\n","\n","StartTraining(model, train_loader, val_loader, EPOCHS, print_every, criterion, optimizer, 'gpu')\n","\n","## SAVE FINAL MODEL\n","model.class_to_idx = train_dataset.class_to_idx\n","state = {\n","            'structure' : ARCH,\n","            'learning_rate': LEARNING_RATE,\n","            'epochs': EPOCHS,\n","            'hidden_layers':hidden_layers,\n","            'state_dict':model.state_dict(),\n","            'class_to_idx':model.class_to_idx\n","}\n","torch.save(state, save_model_to +  ARCH +  '_Final.pth')\n","torch.save(obj=model, f=save_model_to +  ARCH +  '_Final_MODEL.pth')\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qvonR_zf2yw7"},"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.cuda()\n","@torch.no_grad()\n","def get_all_preds(model, dataloader):\n","        \n","        all_preds = torch.tensor([])\n","        all_preds = all_preds.to(device)\n","        all_labels = torch.tensor([])\n","        all_labels = all_labels.to(device)\n","\n","        for data, target in dataloader:\n","            input = data.to(device)\n","            target = target.to(device)\n","\n","            with torch.no_grad():\n","                output = model(input)\n","\n","            all_preds = torch.cat(\n","                (all_preds, output)\n","                ,dim=0\n","            )\n","            all_labels = torch.cat(\n","                (all_labels, target)\n","                ,dim=0\n","            )\n","\n","        return all_preds, all_labels\n","    \n","def get_num_correct(preds, labels):\n","        return preds.argmax(dim=1).eq(labels).sum().item()\n","\n","\n","with torch.no_grad():\n","    model.eval()\n","    test_preds, test_labels = get_all_preds(model,test_loader)\n","\n","    preds_correct = get_num_correct(test_preds.cuda(), test_labels.cuda())\n","    print('total correct:', preds_correct)\n","    print('accuracy:')\n","    print(((preds_correct / (len(test_loader.dataset))) * 100))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kjFcCtveeog6"},"source":["# 8. Exercise VII: Generating Confusion Matrix\n","\n","Use the confusion_matrix function to generate the confusion matrix, then plot it using matplotlib."]},{"cell_type":"code","metadata":{"id":"60mDU9MHfJQS"},"source":["### YOUR CODE HERE ###"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-R4Lbl8GfG4O"},"source":["# Solution"]},{"cell_type":"code","metadata":{"id":"lPs7HX3xe_vD"},"source":["def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n","    if normalize:\n","        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n","        #percentage: \n","        cm = cm.astype('float') * 100\n","        # add percentage sign\n","\n","    mycm = plt.imshow(cm, interpolation='nearest', cmap=cmap)\n","    mycm.set_clim([0,100])\n","    cbar = plt.colorbar(mycm, shrink=0.82, ticks=list(range(0, 120, 20)))\n","    cbar.ax.set_yticklabels(['0', '20', '40', '60', '80', '100'])  # vertically oriented colorbar\n","\n","    tick_marks = np.arange(len(classes))\n","    plt.xticks(tick_marks, classes, rotation=45,  ha=\"right\")\n","    \n","    plt.yticks(tick_marks, classes)\n","\n","    fmt = '.2f' if normalize else 'd'\n","    thresh = cm.max() / 2.\n","    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n","        plt.text(j, i, str(format(cm[i, j], fmt)) + \"%\", horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n","        \n","\n","    plt.rcParams['font.family'] = \"sans-serif\"\n","    plt.rcParams['font.sans-serif'] = \"Arial\"\n","    plt.rcParams.update({'font.size': 12})\n","    plt.ylabel('True class', fontsize=17, fontweight='bold')\n","    plt.xlabel('Predicted class', fontsize=17, fontweight='bold')\n","\n","import itertools\n","\n","cmt = torch.zeros(10, 10, dtype=torch.int32) #10 is the number of classes\n","\n","stacked = torch.stack(\n","    (\n","        test_labels\n","        ,test_preds.argmax(dim=1)\n","    )\n","    ,dim=1\n",")\n","\n","for p in stacked:\n","    tl, pl = p.tolist()\n","    tl = int(tl)\n","    pl = int(pl)\n","    cmt[tl, pl] = cmt[tl, pl] + 1\n","\n","#Plot CM\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import confusion_matrix\n","\n","cm = confusion_matrix(test_labels.cpu(), test_preds.argmax(dim=1).cpu())\n","print(cm)\n","\n","plt.figure(figsize=(12, 12))\n","plot_confusion_matrix(cm, test_dataset.classes, True, 'Confusion matrix', cmap=plt.cm.Blues)\n","plt.savefig(save_model_to + 'confusionMatrix.eps', format='eps', bbox_inches='tight')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vuQgrxQufzPl"},"source":["# 9. Exercise VIII: Generating Explanations With LIME\n","\n","In this exercise you are required to use LIME in order to generate explanations for the classification of the image located under '/content/dataset/segmented/test/Apple___Black_rot/image (475)_cropped_1.JPG'.\n","\n","Generate the explanations using LIME with the default Quickshift segmentation algorithm, then change the segmentation algorithm to Compact-Watershed and compare the results."]},{"cell_type":"code","metadata":{"id":"diOOEbb8hlmx","cellView":"form"},"source":["from lime import lime_image\n","from skimage import io\n","from skimage import img_as_ubyte\n","from skimage.segmentation import mark_boundaries\n","\n","\n","#@title Define Prerequisite Functions\n","Pretrainedmodel =  model\n","def get_PCNN_image(path):\n","  image = cv2.imread(path)\n","  image = cv2.resize(image, (226,226))\n","  return image\n","\n","PerturbationImgs = []\n","\n","def batch_predictPDCNN(images):\n","    Pretrainedmodel.eval()\n","    batch = torch.stack(tuple(preprocess_transform(i) for i in images), dim=0)\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    Pretrainedmodel.to(device)\n","    batch = batch.to(device)\n","    logits = Pretrainedmodel(batch)\n","    probs = F.softmax(logits, dim=1)\n","\n","    for image in images:\n","      PerturbationImgs.append(image)\n","\n","    return probs.detach().cpu().numpy()\n","\n","def get_preprocess_transform():\n","    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                                    std=[0.229, 0.224, 0.225])     \n","    transf = transforms.Compose([\n","        transforms.ToTensor(),\n","        normalize\n","    ])    \n","\n","    return transf    \n","\n","preprocess_transform = get_preprocess_transform()\n","\n","explainerPCNN = lime_image.LimeImageExplainer()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VjDpl_0Mk6xF"},"source":["### WRITE YOUR CODE HERE ###"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KCZ7IVa5h2a1"},"source":["# Solution"]},{"cell_type":"code","metadata":{"id":"DYZ_QJWN6UbT"},"source":["USING QUICKSHIFT:"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h9OsnU9Zh7a5"},"source":["image2explain = '/content/dataset/segmented/test/Apple___Cedar_apple_rust/image (268)_cropped_1.JPG'\n","PCNNimg = get_PCNN_image(image2explain)\n","PCNNimg = cv2.cvtColor(PCNNimg, cv2.COLOR_BGR2RGB)\n","explanationPCNN = explainerPCNN.explain_instance(PCNNimg, \n","                                         batch_predictPDCNN, top_labels=5, hide_color=0, num_samples=1000)\n","\n","tempPCNN, maskPCNN = explanationPCNN.get_image_and_mask(explanationPCNN.top_labels[0], positive_only=True, num_features=1, hide_rest=False)\n","\n","\n","\n","\n","\n","### GENERATE SUPERPIXEL\n","fig, (ax1) = plt.subplots(1, 1, figsize=(5,5))\n","ax1.bbox_inches='tight'\n","ax1.pad_inches = 0\n","ax1.axis('off')\n","plt.subplots_adjust(wspace=0, hspace=0)\n","plt.imshow(mark_boundaries(tempPCNN, explanationPCNN.segments))\n","\n","\n","\n","\n","###############################\n","## SUPER PIXEL PERTURPATIONS ##\n","###############################\n","from matplotlib import gridspec\n","## VISUALIZE SOME PERTURBATIONS\n","# create a figure\n","fig = plt.figure()\n","# to change size of subplot's\n","fig.set_figheight(5)\n","# set width of each subplot as 8\n","fig.set_figwidth(15)\n","\n","# create grid for different subplots\n","spec = gridspec.GridSpec(ncols=5, nrows=2, wspace=0.1, hspace=0.1)\n","\n","print(\"PERTURBATIONS:\")\n","i=0\n","for perturbationImg in PerturbationImgs:\n","    p = fig.add_subplot(spec[i])\n","    p.axis('off')\n","    p.imshow(perturbationImg)\n","    i = i + 1\n","    if i > 9:\n","      break\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4OjoVN79kpFG"},"source":["#######################\n","## SHOW  EXPLANATION ##\n","#######################\n","print(\"FINAL EXPLANATION:\")\n","tempCNNP, maskCNNP = explanationPCNN.get_image_and_mask(explanationPCNN.top_labels[0], positive_only=False, num_features=5, hide_rest=False)\n","fig, (ax1) = plt.subplots(1, 1, figsize=(5,5))\n","ax1.bbox_inches='tight'\n","ax1.pad_inches = 0\n","ax1.axis('off')\n","plt.subplots_adjust(wspace=0, hspace=0)\n","plt.imshow(mark_boundaries(tempCNNP, maskCNNP))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bEMAjmwC6Rtg"},"source":["USING COMPACT-WATERSHED:"]},{"cell_type":"code","metadata":{"id":"9ZgwtRlz6W9N"},"source":["from skimage.segmentation import watershed, quickshift, slic\n","from skimage.color import rgb2gray\n","from lime.wrappers.scikit_image import SegmentationAlgorithm\n","from skimage.filters import sobel\n","\n","def get_PCNN_image(path):\n","  image = cv2.imread(path)\n","  image = cv2.resize(image, (226,226))\n","  return image\n","\n","PCNNimg = get_PCNN_image(image2explain)\n","PCNNimg = cv2.cvtColor(PCNNimg, cv2.COLOR_BGR2RGB)\n","\n","def PCNNcustomSegmentationFunction(image):\n","  gradient = sobel(rgb2gray(image))\n","  segments = watershed(gradient, markers=100, compactness=0.001)\n","  return segments\n","\n","\n","explainerPCNN = lime_image.LimeImageExplainer()\n","explanationPCNN = explainerPCNN.explain_instance(PCNNimg, \n","                                         batch_predictPDCNN, # classification function\n","                                         top_labels=5, \n","                                         hide_color=0, \n","                                         num_samples=1000, segmentation_fn=PCNNcustomSegmentationFunction)\n","\n","\n","tempPCNN, maskPCNN = explanationPCNN.get_image_and_mask(explanationPCNN.top_labels[0], positive_only=True, num_features=2, hide_rest=False)\n","\n","### GENERATE SUPERPIXEL\n","fig, (ax1) = plt.subplots(1, 1, figsize=(5,5))\n","ax1.bbox_inches='tight'\n","ax1.pad_inches = 0\n","ax1.axis('off')\n","plt.subplots_adjust(wspace=0, hspace=0)\n","plt.imshow(mark_boundaries(tempPCNN, explanationPCNN.segments))\n","\n","###############################\n","## SUPER PIXEL PERTURPATIONS ##\n","###############################\n","from matplotlib import gridspec\n","## VISUALIZE SOME PERTURBATIONS\n","# create a figure\n","fig = plt.figure()\n","# to change size of subplot's\n","fig.set_figheight(5)\n","# set width of each subplot as 8\n","fig.set_figwidth(15)\n","\n","# create grid for different subplots\n","spec = gridspec.GridSpec(ncols=5, nrows=2, wspace=0.1, hspace=0.1)\n","\n","print(\"PERTURBATIONS:\")\n","i=0\n","for perturbationImg in PerturbationImgs:\n","    p = fig.add_subplot(spec[i])\n","    p.axis('off')\n","    p.imshow(perturbationImg)\n","    i = i + 1\n","    if i > 9:\n","      break"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"raRAkhRx8EwJ"},"source":["###############################\n","## SHOW POSITIVE EXPLANATION ##\n","###############################\n","tempCNNP, maskCNNP = explanationPCNN.get_image_and_mask(explanationPCNN.top_labels[0], positive_only=False, num_features=5, hide_rest=False)\n","fig, (ax1) = plt.subplots(1, 1, figsize=(5,5))\n","ax1.bbox_inches='tight'\n","ax1.pad_inches = 0\n","ax1.axis('off')\n","plt.subplots_adjust(wspace=0, hspace=0)\n","plt.imshow(mark_boundaries(tempCNNP, maskCNNP))"],"execution_count":null,"outputs":[]}]}