{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1diqC4dBz1xc"
      },
      "source": [
        "# Ready, Steady, Go AI (*Tutorial*)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlQSL0no0uYT"
      },
      "source": [
        "This tutorial is a supplement to the paper, **Ready, Steady, Go AI: A Practical Tutorial on Fundamentals of Artificial Intelligence and Its Applications in Phenomics Image Analysis** (*Patterns, 2021*) by Farid Nakhle and Antoine Harfouche\n",
        "\n",
        "Read the accompanying paper [here](https://doi.org/10.1016/j.patter.2021.100323)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5XoL1aD07Qo"
      },
      "source": [
        "# Table of contents\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SV7BfDp03T2j"
      },
      "source": [
        "* **1. Background**\n",
        "* **2. Downloading Preprocessed Dataset**\n",
        "* **3. Importing the Pretrained Densenet-161 DCNN model**\n",
        "* **4. Generating Explanations with LIME Using Quickshift**\n",
        "* **5. Generating Explanations with LIME Using Compact-Watershed**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7btTy9923b9k"
      },
      "source": [
        "# 1. Background\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVEjYGUW3iMI"
      },
      "source": [
        "**How does LIME work?**\n",
        "\n",
        "The local interpretable model-agnostic explanations (LIME) is an explainable AI algorithm used to open any machine learning (ML) or deep learning (DL) black box model, making their predictions explainable to humans.\n",
        "\n",
        "Simply put, LIME manipulates input data by deleting parts of it and observes the changes in the output of the black box model to be explained. Through the presence or absence of certain parts of data, it monitors their influence on the classification. This way, the algorithm is capable of generating explanations for various data type including text and imaging data.\n",
        "\n",
        "To explain imaging data, LIME starts by partitioning an image into multiple segments that share common characteristics such as pixel intensity, known as superpixels. It then generates perturbations by deleting some superpixels in the image and monitors how they affect the prediction of the blackbox model. Finally, it identifies which areas of the image have been important for classification and thus, generates the explanation by highlighting them.\n",
        "\n",
        "This shows how crucial is the superpixels segmentation for LIME to generate explanations. By default, LIME uses the quickshift algorithm to segment images. \n",
        "\n",
        "We will use LIME to explain our three trained models, RF, the standard DCNN, and the pretrained DCNN, using quickshift and Compact-Watershed superpixel segmentation algorithms.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5n-qgOHeBPuO"
      },
      "source": [
        "# 2. Downloading Preprocessed Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnMTvK_SBSro"
      },
      "source": [
        "As a reminder, we are working with the PlantVillage dataset, originally obtained from [here](http://dx.doi.org/10.17632/tywbtsjrjv.1).\n",
        "For this tutorial, we will be working with a subset of PlantVillage, where we will choose the tomato classes only. We have made the subset available [here](http://dx.doi.org/10.17632/4g7k9wptyd.1). \n",
        "\n",
        "The next code will automatically download the preprocessed dataset.\n",
        "\n",
        "**It is important to note that Colab deletes all unsaved data once the instance is recycled. Therefore, remember to download your results once you run the code.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QkBhoIt66D-"
      },
      "source": [
        "import requests\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "## FEEL FREE TO CHANGE THESE PARAMETERS\n",
        "dataset_url = \"http://faridnakhle.com/pv/tomato-split-cropped-segmented-balanced.zip\"\n",
        "save_data_to = \"/content/dataset/tomato-dataset/\"\n",
        "dataset_file_name = \"tomato-split-cropped-segmented-balanced.zip\"\n",
        "#######################################\n",
        "\n",
        "if not os.path.exists(save_data_to):\n",
        "    os.makedirs(save_data_to)\n",
        "\n",
        "r = requests.get(dataset_url, stream = True, headers={\"User-Agent\": \"Ready, Steady, Go AI\"})\n",
        "\n",
        "print(\"Downloading dataset...\")  \n",
        "\n",
        "with open(save_data_to + dataset_file_name, \"wb\") as file: \n",
        "    for block in r.iter_content(chunk_size = 1024):\n",
        "         if block: \n",
        "             file.write(block)\n",
        "\n",
        "## Extract downloaded zip dataset file\n",
        "print(\"Dataset downloaded\")  \n",
        "print(\"Extracting files...\")  \n",
        "with zipfile.ZipFile(save_data_to + dataset_file_name, 'r') as zip_dataset:\n",
        "    zip_dataset.extractall(save_data_to)\n",
        "\n",
        "## Delete the zip file as we no longer need it\n",
        "os.remove(save_data_to + dataset_file_name)\n",
        "print(\"All done!\")  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZFcJ7w8ESfL"
      },
      "source": [
        "# 3. Importing the Pretrained Densenet-161 DCNN model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3n7Us9vq6rGd"
      },
      "source": [
        "import argparse\n",
        "import os\n",
        "import time\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms, models\n",
        "from PIL import Image\n",
        "from collections import OrderedDict\n",
        "import json\n",
        "!pip install lime\n",
        "from lime import lime_image\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezSRC1Zh-CN3"
      },
      "source": [
        "## YOU CAN CHANGE THESE VARIABLES    \n",
        "EPOCHS = 100\n",
        "BATCH_SIZE = 20\n",
        "LEARNING_RATE = 0.0001\n",
        "data_dir = '/content/dataset/tomato-dataset/'\n",
        "save_checkpoints = True\n",
        "save_model_to = '/content/output/'\n",
        "!mkdir /content/output/\n",
        "IMG_SIZE = 220\n",
        "NUM_WORKERS = 1\n",
        "using_gpu = torch.cuda.is_available()\n",
        "print_every = 300\n",
        "ARCH = 'densenet161'\n",
        "######################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZ8_YQVLECIj"
      },
      "source": [
        "Next, we will define a function that creates a data loader for all of our sets (i.e., training, validation, and testing)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLPvgdyPD9WI"
      },
      "source": [
        "def data_loader(root, batch_size=256, workers=1, pin_memory=True):\n",
        "    traindir = os.path.join(root, 'train')\n",
        "    valdir = os.path.join(root, 'val')\n",
        "    testdir = os.path.join(root, 'test')\n",
        "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                     std=[0.229, 0.224, 0.225])\n",
        "\n",
        "    train_dataset = datasets.ImageFolder(\n",
        "        traindir,\n",
        "        transforms.Compose([\n",
        "            transforms.Resize(size=(IMG_SIZE, IMG_SIZE)),\n",
        "            transforms.ToTensor(),\n",
        "            normalize\n",
        "        ])\n",
        "    )\n",
        "    val_dataset = datasets.ImageFolder(\n",
        "        valdir,\n",
        "        transforms.Compose([\n",
        "            transforms.Resize(size=(IMG_SIZE, IMG_SIZE)),\n",
        "            transforms.ToTensor(),\n",
        "            normalize\n",
        "        ])\n",
        "    )\n",
        "    test_dataset = datasets.ImageFolder(\n",
        "        testdir,\n",
        "        transforms.Compose([\n",
        "            transforms.Resize(size=(IMG_SIZE, IMG_SIZE)),\n",
        "            transforms.ToTensor(),\n",
        "            normalize\n",
        "        ])\n",
        "    )\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=workers,\n",
        "        pin_memory=pin_memory,\n",
        "        sampler=None\n",
        "    )\n",
        "    val_loader = torch.utils.data.DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=workers,\n",
        "        pin_memory=pin_memory\n",
        "    )\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=workers,\n",
        "        pin_memory=pin_memory\n",
        "    )\n",
        "    return train_loader, val_loader, test_loader, train_dataset, val_dataset, test_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0phgSLM5EQzT"
      },
      "source": [
        "# Data loading\n",
        "train_loader, val_loader, test_loader, train_dataset, val_dataset, test_dataset = data_loader(data_dir, BATCH_SIZE, NUM_WORKERS, False)\n",
        "print(\"Training Set: \" + str(len(train_loader.dataset)))\n",
        "print(\"Validation Set: \" + str(len(val_loader.dataset)))\n",
        "print(\"Testing Set: \" + str(len(test_loader.dataset)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pg07HqM1EZs4"
      },
      "source": [
        "The next code block is the function that creates our algorithm architecture. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8wUV-4zEgU2"
      },
      "source": [
        "# Freeze parameters so we don't backprop through them\n",
        "hidden_layers = [10240, 1024]\n",
        "def make_model(structure, hidden_layers, lr, preTrained):\n",
        "    if structure==\"densenet161\":\n",
        "        model = models.densenet161(pretrained=preTrained)\n",
        "        input_size = 2208\n",
        "    else:\n",
        "        model = models.vgg16(pretrained=preTrained)\n",
        "        input_size = 25088\n",
        "    output_size = 102\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    classifier = nn.Sequential(OrderedDict([\n",
        "                              ('dropout',nn.Dropout(0.5)),\n",
        "                              ('fc1', nn.Linear(input_size, hidden_layers[0])),\n",
        "                              ('relu1', nn.ReLU()),\n",
        "                              ('fc2', nn.Linear(hidden_layers[0], hidden_layers[1])),\n",
        "                              ('relu2', nn.ReLU()),\n",
        "                              ('fc3', nn.Linear(hidden_layers[1], output_size)),\n",
        "                              ('output', nn.LogSoftmax(dim=1))\n",
        "                              ]))\n",
        "\n",
        "    model.classifier = classifier\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdeuonCMEkC0"
      },
      "source": [
        "Pretrainedmodel = make_model(ARCH, hidden_layers, LEARNING_RATE, True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jIt3EIvkEl7s"
      },
      "source": [
        "# define loss and optimizer\n",
        "criterion = nn.NLLLoss()\n",
        "PretrainedmodelOptimizer = optim.Adam(Pretrainedmodel.classifier.parameters(), lr=LEARNING_RATE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCSed4aDEzgX"
      },
      "source": [
        "Now we have everything setup and ready to start traning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPQdvJty5MgU"
      },
      "source": [
        "**In the next section, we will load our trained model to make our results reproducable. You can change the loading path to use your own instead**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCsksQISn2SL"
      },
      "source": [
        "###########################\n",
        "### DOWNLOAD THE MODELS ###\n",
        "###########################\n",
        "\n",
        "## FEEL FREE TO CHANGE THESE PARAMETERS\n",
        "pretrained_Model_URL = \"http://faridnakhle.com/pv/models/RSGAI_DenseNet.zip\"\n",
        "save_data_to = \"/content/models/\"\n",
        "pretrained_file_name = \"densenet.zip\"\n",
        "#######################################\n",
        "\n",
        "if not os.path.exists(save_data_to):\n",
        "    os.makedirs(save_data_to)\n",
        "\n",
        "print(\"Downloading models...\")  \n",
        "\n",
        "\n",
        "r = requests.get(pretrained_Model_URL, stream = True, headers={\"User-Agent\": \"Ready, Steady, Go AI\"})\n",
        "with open(save_data_to + pretrained_file_name, \"wb\") as file: \n",
        "    for block in r.iter_content(chunk_size = 1024):\n",
        "         if block: \n",
        "             file.write(block)\n",
        "\n",
        "## Extract downloaded zip dataset file\n",
        "print(\"Model downloaded\")  \n",
        "print(\"Extracting files...\")\n",
        "\n",
        "\n",
        "with zipfile.ZipFile(save_data_to + pretrained_file_name, 'r') as zip_dataset:\n",
        "    zip_dataset.extractall(save_data_to)\n",
        "\n",
        "\n",
        "print(\"All done!\")  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFubN2LrFUZH"
      },
      "source": [
        "PRETRAINED_PATH = '/content/models/RSGAI_DenseNet.pth'\n",
        "def loading_checkpoint(path, pretrained):\n",
        "    # Loading the parameters\n",
        "    state = torch.load(path)\n",
        "    LEARNING_RATE = state['learning_rate']\n",
        "    structure = state['structure']\n",
        "    hidden_layers = state['hidden_layers']\n",
        "    epochs = state['epochs']\n",
        "    \n",
        "    # Building the model from checkpoints\n",
        "    model = make_model(structure, hidden_layers, LEARNING_RATE, pretrained)\n",
        "    model.class_to_idx = state['class_to_idx']\n",
        "    model.load_state_dict(state['state_dict'])\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "\n",
        "Pretrainedmodel = loading_checkpoint(PRETRAINED_PATH, True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdVhOobAGncB"
      },
      "source": [
        "# 4. Generating Explanations with LIME"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjYR9Fv8quft"
      },
      "source": [
        "!pip install mahotas\n",
        "import itertools\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import mahotas\n",
        "import cv2\n",
        "import h5py\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from PIL import Image\n",
        "\n",
        "## LIME \n",
        "from lime import lime_image\n",
        "from skimage import io\n",
        "from skimage import img_as_ubyte\n",
        "from skimage.segmentation import mark_boundaries"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHu3TpA1sQDj"
      },
      "source": [
        "image2explain = '/content/dataset/tomato-dataset/test/Tomato___Late_blight/image (1076)_cropped_1.JPG'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0zXIl0e3fsl"
      },
      "source": [
        "Explanations with Pretrained CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4IGT2-gy3kBd"
      },
      "source": [
        "def get_PCNN_image(path):\n",
        "  image = cv2.imread(path)\n",
        "  image = cv2.resize(image, (226,226))\n",
        "  return image\n",
        "\n",
        "PCNNimg = get_PCNN_image(image2explain)\n",
        "PCNNimg = cv2.cvtColor(PCNNimg, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "def get_preprocess_transform():\n",
        "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                    std=[0.229, 0.224, 0.225])     \n",
        "    transf = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        normalize\n",
        "    ])    \n",
        "\n",
        "    return transf    \n",
        "\n",
        "preprocess_transform = get_preprocess_transform()\n",
        "\n",
        "\n",
        "def batch_predictPDCNN(images):\n",
        "    Pretrainedmodel.eval()\n",
        "    batch = torch.stack(tuple(preprocess_transform(i) for i in images), dim=0)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    Pretrainedmodel.to(device)\n",
        "    batch = batch.to(device)\n",
        "    logits = Pretrainedmodel(batch)\n",
        "    probs = F.softmax(logits, dim=1)\n",
        "    return probs.detach().cpu().numpy()\n",
        "\n",
        "\n",
        "explainerPCNN = lime_image.LimeImageExplainer()\n",
        "explanationPCNN = explainerPCNN.explain_instance(PCNNimg, \n",
        "                                         batch_predictPDCNN, # classification function\n",
        "                                        # top_labels=5, \n",
        "                                         hide_color=0, \n",
        "                                         num_samples=5000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HM669mDd4IAq"
      },
      "source": [
        "tempPCNN, maskPCNN = explanationPCNN.get_image_and_mask(explanationPCNN.top_labels[0], positive_only=True, num_features=1, hide_rest=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-D9QaqBn4cGF"
      },
      "source": [
        "###############################\n",
        "##     SHOW  EXPLANATION     ##\n",
        "###############################\n",
        "tempCNNP, maskCNNP = explanationPCNN.get_image_and_mask(explanationPCNN.top_labels[0], positive_only=False, num_features=1, hide_rest=False)\n",
        "fig, (ax1) = plt.subplots(1, 1, figsize=(5,5))\n",
        "ax1.bbox_inches='tight'\n",
        "ax1.pad_inches = 0\n",
        "ax1.axis('off')\n",
        "plt.subplots_adjust(wspace=0, hspace=0)\n",
        "plt.imshow(mark_boundaries(tempCNNP, maskCNNP))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}