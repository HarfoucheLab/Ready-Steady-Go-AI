{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"RSG_Downsample_KNN.ipynb","provenance":[],"collapsed_sections":["1diqC4dBz1xc"],"toc_visible":true,"authorship_tag":"ABX9TyPj0EiobVoU0WiW5lfhMXz3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"1diqC4dBz1xc"},"source":["# Ready, Steady, Go AI (*Tutorial*)"]},{"cell_type":"markdown","metadata":{"id":"MlQSL0no0uYT"},"source":["This tutorial is a supplement to the paper, **Ready, Steady, Go AI: A Practical Tutorial on Explainable Artificial Intelligence and Its Applications in Plant Digital Phenomics** (submitted to *Patterns, 2021*) by Farid Nakhle and Antoine Harfouche\n","\n","Read the accompanying paper [here](https://doi.org)."]},{"cell_type":"markdown","metadata":{"id":"X5XoL1aD07Qo"},"source":["# Table of contents\n"]},{"cell_type":"markdown","metadata":{"id":"SV7BfDp03T2j"},"source":["* **1. Background**\n","* **2. Downloading Segmented Images**\n","* **3. Downsampling the Yellow Leaf Curl Class**"]},{"cell_type":"markdown","metadata":{"id":"7btTy9923b9k"},"source":["# 1. Background\n"]},{"cell_type":"markdown","metadata":{"id":"iVEjYGUW3iMI"},"source":["**Why do we need to balance a dataset?**\n","\n","Data imbalance refers to an unequal distribution of classes within a dataset. In such scenario, a classification model could become biased, inaccurate and might produce unsatisfactory results. Therefore, we balance the dataset either by oversampling the minority class or undersampling the majority classes. To demonstrate the two scenarios, both oversampling and undersampling will be applied. Here, we will downsample the yellow leaf curl class in the training set using K-nearest neighbors (KNN).\n","\n","**What is KNN?**\n","\n","Like oversampling, undersampling is also designed to balance the class distribution in an imbalanced dataset. However, in contrast to oversampling, undersampling techniques delete data from the majority classes to balance the distribution. \n","\n","KNN is an ML algorithm that calculates feature similarity between its training data to predict values for new, previously unseen data. When given a new input, KNN finds k (a user-predefined number) most resembling data (nearest neighbors) using similarity metrics, such as the Euclidean distance. Based on the majority class of those similar cases, the algorithm classifies the new input. \n","\n","We will use KNN to discover similarities of leaves in the yellow leaf curl virus class, deleting images with redundant features, ultimately undersampling it to 1500 images.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"5n-qgOHeBPuO"},"source":["# 2. Downloading Segmented Images\n"]},{"cell_type":"markdown","metadata":{"id":"OnMTvK_SBSro"},"source":["As a reminder, we are working with the PlantVillage dataset, originally obtained from [here](http://dx.doi.org/10.17632/tywbtsjrjv.1).\n","For this tutorial, we will be working with a subset of PlantVillage, where we will choose the tomato classes only. We have made the subset available [here](http://dx.doi.org/10.17632/4g7k9wptyd.1). \n","\n","The next code will automatically download the dataset segmented with SegNet.\n","\n","**It is important to note that Colab deletes all unsaved data once the instance is recycled. Therefore, remember to download your results once you run the code.**"]},{"cell_type":"code","metadata":{"id":"2QkBhoIt66D-","colab":{"base_uri":"https://localhost:8080/"},"outputId":"52222dd1-df55-475d-c438-3f31212faf2a"},"source":["import requests\n","import os\n","import zipfile\n","\n","## FEEL FREE TO CHANGE THESE PARAMETERS\n","dataset_url = \"http://faridnakhle.com/pv/tomato-split-cropped-segmented.zip\"\n","save_data_to = \"/content/dataset/tomato-segmented/\"\n","dataset_file_name = \"tomato-segmented.zip\"\n","#######################################\n","\n","if not os.path.exists(save_data_to):\n","    os.makedirs(save_data_to)\n","\n","r = requests.get(dataset_url, stream = True, headers={\"User-Agent\": \"Ready, Steady, Go AI\"})\n","\n","print(\"Downloading dataset...\")  \n","\n","with open(save_data_to + dataset_file_name, \"wb\") as file: \n","    for block in r.iter_content(chunk_size = 1024):\n","         if block: \n","             file.write(block)\n","\n","## Extract downloaded zip dataset file\n","print(\"Dataset downloaded\")  \n","print(\"Extracting files...\")  \n","with zipfile.ZipFile(save_data_to + dataset_file_name, 'r') as zip_dataset:\n","    zip_dataset.extractall(save_data_to)\n","\n","## Delete the zip file as we no longer need it\n","os.remove(save_data_to + dataset_file_name)\n","print(\"All done!\")  \n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading dataset...\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"MZFcJ7w8ESfL"},"source":["#  3. Downsampling the Yellow Leaf Curl Class"]},{"cell_type":"code","metadata":{"id":"K7zYUC9vDHU_"},"source":["from sklearn.neighbors import NearestNeighbors\n","from glob import glob\n","\n","import numpy as np\n","import scipy.sparse as sp\n","from keras.applications import VGG19\n","from keras.applications.vgg19 import preprocess_input\n","from keras.engine import Model\n","from keras.preprocessing import image\n","import numpy as np\n","import os\n","\n","\n","img_dir = \"/content/dataset/tomato-segmented/train/Tomato___Tomato_Yellow_Leaf_Curl_Virus/*\"\n","targetLimit = 1500\n","deleteImages = True\n","\n","def SaveFile(arr, filename):\n","    with open(filename, 'w') as filehandle:\n","        for listitem in arr:\n","            filehandle.write(str(listitem) + \"\\n\")\n","\n","\n","def vectorize_all(files, model, px=224, n_dims=512, batch_size=512):\n","    min_idx = 0\n","    max_idx = min_idx + batch_size\n","    total_max = len(files)\n","    if (max_idx > total_max):\n","        max_idx = total_max\n","    \n","    preds = sp.lil_matrix((len(files), n_dims))\n","\n","    print(\"Total: {}\".format(len(files)))\n","    while min_idx < total_max - 1:\n","        print(min_idx)\n","        X = np.zeros(((max_idx - min_idx), px, px, 3))\n","        # For each file in batch, \n","        # load as row into X\n","        i = 0\n","        for i in range(min_idx, max_idx):\n","            file = files[i]\n","            try:\n","                img = image.load_img(file, target_size=(px, px))\n","                img_array = image.img_to_array(img)\n","                X[i - min_idx, :, :, :] = img_array\n","            except Exception as e:\n","                print(e)\n","        max_idx = i\n","        X = preprocess_input(X)\n","        these_preds = model.predict(X)\n","        shp = ((max_idx - min_idx) + 1, n_dims)\n","        preds[min_idx:max_idx + 1, :] = these_preds.reshape(shp)\n","        min_idx = max_idx\n","        max_idx = np.min((max_idx + batch_size, total_max))\n","    return preds\n","\n","def vectorizeOne(path, model):\n","    img = image.load_img(path, target_size=(224, 224))\n","    x = image.img_to_array(img)\n","    x = np.expand_dims(x, axis=0)\n","    x = preprocess_input(x)\n","    pred = model.predict(x)\n","    return pred.ravel()\n","\n","def findSimilar(vec, knn, filenames, n_neighbors=6):\n","    if n_neighbors >= len(filenames):\n","        print(\"Error. number of neighbours should be less than the number of images.\")\n","    else:\n","        n_neighbors = n_neighbors + 1\n","        dist, indices = knn.kneighbors(vec.reshape(1, -1), n_neighbors=n_neighbors)\n","        dist, indices = dist.flatten(), indices.flatten()\n","        similarList = [(filenames[indices[i]], dist[i]) for i in range(len(indices))]\n","        del similarList[0]\n","        #similarImages.sort(reverse=True, key=lambda tup: tup[1])\n","        return similarList\n","\n","files = glob(img_dir)\n","nbrOfImages2Delete = len(files) - targetLimit\n","\n","if (nbrOfImages2Delete > 0):\n","\n","    imgToSearchFor = files[0]\n","\n","    base_model = VGG19(weights='imagenet')\n","    model = Model(inputs=base_model.input, outputs=base_model.get_layer('fc1').output)\n","    vecs = vectorize_all(files, model, n_dims=4096)\n","\n","    knn = NearestNeighbors(metric='cosine', algorithm='brute')\n","    knn.fit(vecs)\n","\n","    vec = vectorizeOne(imgToSearchFor, model)\n","    similarImages = findSimilar(vec, knn, files, nbrOfImages2Delete)\n","    print(similarImages)\n","    SaveFile(similarImages, \"deletedImages.txt\")\n","\n","    if deleteImages:\n","        for i in range(0, len(similarImages)):\n","            if os.path.exists(similarImages[i][0]):\n","                os.remove(similarImages[i][0])\n","    print(\"Balancing done. A list of deleted images can be found in deletedImages.txt\")\n","else:\n","    print(\"nothing to delete\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4x2ZhIJ8Jc9_"},"source":["Let's re-count the files in the folder"]},{"cell_type":"code","metadata":{"id":"RRefF9BpJcik","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618148201259,"user_tz":-120,"elapsed":651,"user":{"displayName":"Farid Nakhle","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhUlVurX8XdrGra-Jpp6WlS33hxKQHBtkwXwhhwYQ=s64","userId":"13288215553045444657"}},"outputId":"c73cc0f4-f8b5-4682-8fc3-03d51d843a0e"},"source":["files = glob(img_dir)\n","print(len(files))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["1500\n"],"name":"stdout"}]}]}