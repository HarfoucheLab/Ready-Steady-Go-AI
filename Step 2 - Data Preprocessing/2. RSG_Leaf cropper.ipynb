{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1diqC4dBz1xc"
      },
      "source": [
        "# Ready, Steady, Go AI (*Tutorial*)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlQSL0no0uYT"
      },
      "source": [
        "This tutorial is a supplement to the paper, **Ready, Steady, Go AI: A Practical Tutorial on Fundamentals of Artificial Intelligence and Its Applications in Phenomics Image Analysis** (*Patterns, 2021*) by Farid Nakhle and Antoine Harfouche\n",
        "\n",
        "Read the accompanying paper [here](https://doi.org/10.1016/j.patter.2021.100323)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5XoL1aD07Qo"
      },
      "source": [
        "# Table of contents\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SV7BfDp03T2j"
      },
      "source": [
        "* **1. Background**\n",
        "* **2. Downloading Annotated Images**\n",
        "* **3. Training YOLOv3**\n",
        "* **4. Downloading The Split Dataset**\n",
        "* **5. Cropping Images**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7btTy9923b9k"
      },
      "source": [
        "# 1. Background\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVEjYGUW3iMI"
      },
      "source": [
        "**Why do we need to crop?**\n",
        "\n",
        "Datasets will sometimes include images that depict more than one object, typically belonging to background objects that are not of interest for an AI algorithm to learn. This could negatively affect its training and can be solved by cropping images, keeping only important parts of it. However, this process can take a considerable amount of time and effort. We show how cropping can be automated using the you only look once (YOLO) algorithm\n",
        "\n",
        "**What is YOLOv3?**\n",
        "\n",
        "YOLO is an algorithm that devides an input image into an S Ã— S grid and, for every object in the image, the grid cell associated with the center of it is responsible for its detection. This is done by having the cell predict a number of bounding boxes to surround the object. \n",
        "\n",
        "As cropping helps classification algorithms concentrate on the regions of interest within images, we exploit the object detection and localization feature of YOLOv3 to automatically detect, localize, and crop the leaves in images.\n",
        "\n",
        "1000 images, selected randomly from our 10 tomato were manually annotated by drawing bounding boxes around the leaves using the MakeSense annotation tool (https://makesense.ai).\n",
        "\n",
        "We will leverage the annotated images here to train YOLOv3.\n",
        "\n",
        "Then, we will use the output model to crop the rest of the images.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5n-qgOHeBPuO"
      },
      "source": [
        "# 2. Downloading Annotated Images\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnMTvK_SBSro"
      },
      "source": [
        "As a reminder, we are working with the PlantVillage dataset, originally obtained from [here](http://dx.doi.org/10.17632/tywbtsjrjv.1).\n",
        "For this tutorial, we will be working with a subset of PlantVillage, where we will choose the tomato classes only. We have made the subset available [here](http://dx.doi.org/10.17632/4g7k9wptyd.1). \n",
        "\n",
        "The next code will automatically download 1000 images, specifically annotated to train YOLO.\n",
        "\n",
        "**It is important to note that Colab deletes all unsaved data once the instance is recycled. Therefore, remember to download your results once you run the code.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QkBhoIt66D-"
      },
      "source": [
        "import requests\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "## FEEL FREE TO CHANGE THESE PARAMETERS\n",
        "dataset_url = \"http://faridnakhle.com/pv/tomato-YOLOv3-annotations.zip\"\n",
        "save_data_to = \"/content/dataset/YOLO_Training\"\n",
        "dataset_file_name = \"YOLO.zip\"\n",
        "#######################################\n",
        "\n",
        "if not os.path.exists(save_data_to):\n",
        "    os.makedirs(save_data_to)\n",
        "\n",
        "r = requests.get(dataset_url, stream = True, headers={\"User-Agent\": \"Ready, Steady, Go AI\"})\n",
        "\n",
        "print(\"Downloading dataset...\")  \n",
        "\n",
        "with open(save_data_to + dataset_file_name, \"wb\") as file: \n",
        "    for block in r.iter_content(chunk_size = 1024):\n",
        "         if block: \n",
        "             file.write(block)\n",
        "\n",
        "## Extract downloaded zip dataset file\n",
        "print(\"Dataset downloaded\")  \n",
        "print(\"Extracting files...\")  \n",
        "with zipfile.ZipFile(save_data_to + dataset_file_name, 'r') as zip_dataset:\n",
        "    zip_dataset.extractall(save_data_to)\n",
        "\n",
        "## Delete the zip file as we no longer need it\n",
        "os.remove(save_data_to + dataset_file_name)\n",
        "print(\"All done!\")  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZFcJ7w8ESfL"
      },
      "source": [
        "# 3. Training YOLOv3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YeGGr_l4ET6h"
      },
      "source": [
        "Our code will be based on the YOLOv3 implementation of Ultralytics LLC, open source and freely available for redistribution under the [GPL-3.0 license](https://choosealicense.com/licenses/gpl-3.0/). \n",
        "For more information please visit https://github.com/ultralytics/yolov3."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljFSQu3E09Bn"
      },
      "source": [
        "## CLONE THEIR REPO and install all pre-requisites\n",
        "!pip uninstall torch -y\n",
        "#!pip install torch==1.8.2 torchvision==0.9.2 torchaudio==0.8.2 --extra-index-url https://download.pytorch.org/whl/lts/1.8/cu102\n",
        "!pip install torch==1.7.1+cu101 torchvision==0.8.2+cu101 torchaudio==0.7.2 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip uninstall numpy\n",
        "!pip3 install numpy==1.18.5\n",
        "\n",
        "\n",
        "!git clone --depth 1 --branch v9.0 https://github.com/ultralytics/yolov3\n",
        "%cd yolov3\n",
        "%pip install -qr requirements.txt "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzwBwk7tXmv9"
      },
      "source": [
        "Next, we will import Torch Software Framework. It is recommended to use a GPU instance for faster training. By default, this notebook runs on GPU. If you would like to change the instance type, check Colab docs [here](https://colab.research.google.com/notebooks/gpu.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "taPQtg4eXU9s"
      },
      "source": [
        "import torch\n",
        "from IPython.display import Image\n",
        "\n",
        "print('Importing Torch software framework is complete. Using torch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0) if torch.cuda.is_available() else 'CPU'))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVbs9W9u2sa1"
      },
      "source": [
        "**NB: To make running this notebook faster and our results easily reproducable, we made our trained model available and we will load it after this section. Thus, you might skip this next code block**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pM27yLQFYGXW"
      },
      "source": [
        "print(\"Starting Training\")\n",
        "!python train.py --img 224 --batch 16 --epochs 300 --data /content/dataset/YOLO_Training/RSG_Tomato.yaml --weights yolov3.pt --nosave --cache\n",
        "print(\"Training Done\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIlD_sq1zCVN"
      },
      "source": [
        "The best generated model is now saved /content/yolov3/runs/expX/weights/best.pt\n",
        "where X in expX represents the experiment number (how many times the code was run)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8ciiDPa0KxR"
      },
      "source": [
        "**In the next section, we will load our trained model to make our results reproducable. You can change the loading path to use your own instead**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxiSr08w0f5K"
      },
      "source": [
        "##########################\n",
        "### DOWNLOAD THE MODEL ###\n",
        "##########################\n",
        "\n",
        "## FEEL FREE TO CHANGE THESE PARAMETERS\n",
        "model_URL = \"http://faridnakhle.com/pv/models/YOLOv3.zip\"\n",
        "save_data_to = \"/content/models/\"\n",
        "model_file_name = \"yolo.zip\"\n",
        "#######################################\n",
        "\n",
        "if not os.path.exists(save_data_to):\n",
        "    os.makedirs(save_data_to)\n",
        "\n",
        "print(\"Downloading model...\")  \n",
        "\n",
        "r = requests.get(model_URL, stream = True, headers={\"User-Agent\": \"Ready, Steady, Go AI\"})\n",
        "with open(save_data_to + model_file_name, \"wb\") as file: \n",
        "    for block in r.iter_content(chunk_size = 1024):\n",
        "         if block: \n",
        "             file.write(block)\n",
        "\n",
        "## Extract downloaded zip dataset file\n",
        "print(\"Model downloaded\")  \n",
        "print(\"Extracting files...\")\n",
        "\n",
        "with zipfile.ZipFile(save_data_to + model_file_name, 'r') as zip_dataset:\n",
        "    zip_dataset.extractall(save_data_to)\n",
        "print(\"All done!\")  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UeKXxlSnuym"
      },
      "source": [
        "# 4. Downloading The Split Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWpFElhYnwVR"
      },
      "source": [
        "import requests\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "## FEEL FREE TO CHANGE THESE PARAMETERS\n",
        "dataset_url = \"http://faridnakhle.com/pv/tomato-split-80-10-10.zip\"\n",
        "save_data_to = \"/content/dataset/tomato-split/\"\n",
        "dataset_file_name = \"tomato-split.zip\"\n",
        "#######################################\n",
        "\n",
        "if not os.path.exists(save_data_to):\n",
        "    os.makedirs(save_data_to)\n",
        "\n",
        "r = requests.get(dataset_url, stream = True, headers={\"User-Agent\": \"Ready, Steady, Go AI\"})\n",
        "\n",
        "print(\"Downloading dataset...\")  \n",
        "\n",
        "with open(save_data_to + dataset_file_name, \"wb\") as file: \n",
        "    for block in r.iter_content(chunk_size = 1024):\n",
        "         if block: \n",
        "             file.write(block)\n",
        "\n",
        "## Extract downloaded zip dataset file\n",
        "print(\"Dataset downloaded\")  \n",
        "print(\"Extracting files...\")  \n",
        "with zipfile.ZipFile(save_data_to + dataset_file_name, 'r') as zip_dataset:\n",
        "    zip_dataset.extractall(save_data_to)\n",
        "\n",
        "## Delete the zip file as we no longer need it\n",
        "os.remove(save_data_to + dataset_file_name)\n",
        "print(\"All done!\")  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eM3R2qwPoSHQ"
      },
      "source": [
        "# 5. Cropping Images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tT_YvFlzvGk"
      },
      "source": [
        "Next, we are going to implement a custom code that locates the bounding box generated by YOLO, and crop the image accordingly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPO8GCfBz2ux"
      },
      "source": [
        "import os\n",
        "import cv2\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# function for cropping each detection and saving as new image\n",
        "def crop_objects(img, data, path):\n",
        "    boxes, scores, classes, num_objects = data\n",
        "    #create dictionary to hold count of objects for image name\n",
        "    for i in range(len(num_objects)):\n",
        "        # get count of class for part of image name\n",
        "        class_index = int(classes[i])\n",
        "        # get box coords\n",
        "        xmin, ymin, xmax, ymax = boxes[i]\n",
        "        # crop detection from image\n",
        "        cropped_img = img[int(ymin)-5:int(ymax)+5, int(xmin)-5:int(xmax)+5]\n",
        "        # construct image name and join it to path for saving crop properly\n",
        "        img_name =  'cropped_img.png'\n",
        "        img_path = os.path.join(path, img_name )\n",
        "        # save image\n",
        "        cv2.imwrite(img_path, cropped_img)\n",
        "\n",
        "def crop_object(img, coords, img_path):\n",
        "    # get box coords\n",
        "    xmin = int(coords[0])\n",
        "    ymin = int(coords[1])\n",
        "    xmax = int(coords[2])\n",
        "    ymax = int(coords[3])\n",
        "    # crop detection from image\n",
        "    cropped_img = img[ymin:ymax, xmin:xmax]\n",
        "    # save image\n",
        "    cv2.imwrite(img_path, cropped_img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "poJw7SXI1C22"
      },
      "source": [
        "Now that we have our cropping function, we will write 3 more functions to: (i) plot the grid on the image; (ii) plot the image borders; and (iii) plot the map overlay on the image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJ0PCIpR1fEm"
      },
      "source": [
        "def plot_grid(img, line_color=(0, 255, 0), thickness=1, type_=cv2.LINE_AA, pxstep=20, pystep=20):\n",
        "    x = pystep\n",
        "    y = pxstep\n",
        "\n",
        "    while x < img.shape[1]:\n",
        "        cv2.line(img, (x, 0), (x, img.shape[0]), color=line_color, lineType=type_, thickness=thickness)\n",
        "        x += pystep\n",
        "\n",
        "    while y < img.shape[0]:\n",
        "        cv2.line(img, (0, y), (img.shape[1], y), color=line_color, lineType=type_, thickness=thickness)\n",
        "        y += pxstep"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCWaQ3_i1nRv"
      },
      "source": [
        "def plot_borders(img, line_color=(0, 255, 0), thickness=1):\n",
        "    cv2.rectangle(img,(0 ,0),(img.shape[1]-thickness,img.shape[0]-thickness), line_color, thickness)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7SHPAba1Uqi"
      },
      "source": [
        "def myround(x, base=5):\n",
        "    return base * round(x/base)\n",
        "def plot_overlay(x, img, color, alpha,\n",
        " pxstep=20, pystep=20):\n",
        "    overlay = img.copy()\n",
        "    x0, x1, x2, x3 = int(x[0]), int(x[1]), int(x[2]), int(x[3])\n",
        "\n",
        "    x0 = myround(x0,pystep)\n",
        "    x1 = myround(x1,pxstep)\n",
        "    x2 = myround(x2,pystep)\n",
        "    x3 = myround(x3,pxstep)\n",
        "\n",
        "    c1, c2 = (x0, x1), (x2, x3)\n",
        "    cv2.rectangle(overlay, c1, c2, color, -1)\n",
        "    # apply the overlay\n",
        "    cv2.addWeighted(overlay, alpha, img, 1 - alpha, 0, img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1NYnn2A0LJY"
      },
      "source": [
        "Next, we will write a custom detection function that uses Ultralytics code to crop all of our dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxAroGz202r1"
      },
      "source": [
        "!cd /content/yolov3/\n",
        "import argparse\n",
        "import time\n",
        "from pathlib import Path\n",
        "\n",
        "import cv2\n",
        "import torch\n",
        "import torch.backends.cudnn as cudnn\n",
        "from numpy import random\n",
        "\n",
        "from models.experimental import attempt_load\n",
        "from utils.datasets import LoadStreams, LoadImages\n",
        "from utils.general import check_img_size, non_max_suppression, apply_classifier, scale_coords, xyxy2xywh, \\\n",
        "    strip_optimizer, set_logging, increment_path\n",
        "from utils.plots import plot_one_box\n",
        "from utils.torch_utils import select_device, load_classifier, time_synchronized\n",
        "\n",
        "import glob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_90BMkJ13J0"
      },
      "source": [
        "def detect(dataset_dir = '/content/dataset/tomato-split/', model_path='/content/yolov3/runs/train/exp/best.pt'):\n",
        "    \n",
        "    save_txt, imgsz = False, 224\n",
        "    weights = model_path\n",
        "    projectP = 'runs/detect'\n",
        "    projectNameP = 'exp'\n",
        "    save_img = True\n",
        "    view_img = True\n",
        "\n",
        "    save_dir = Path(increment_path(Path(projectP) / projectNameP, False))  # increment run\n",
        "    (save_dir / 'labels' if save_txt else save_dir).mkdir(parents=True, exist_ok=True)  # make dir\n",
        "\n",
        "    #loop over train, val, and test set\n",
        "    trainTestVarDirs = glob.glob(dataset_dir + \"*\")\n",
        "    for setDir in trainTestVarDirs:\n",
        "      splitDir = os.path.basename(setDir)\n",
        "      setClasses = glob.glob(setDir + \"/*\")\n",
        "      for setClass in setClasses:\n",
        "        # Directories\n",
        "        classDir = os.path.basename(setClass)\n",
        "        finalSaveDir = os.path.join(save_dir, splitDir, classDir)\n",
        "        Path(finalSaveDir).mkdir(parents=True, exist_ok=True)\n",
        "        source = setClass\n",
        "        \n",
        "\n",
        "        # Initialize\n",
        "        set_logging()\n",
        "        device = select_device('0')\n",
        "        half = device.type != 'cpu'  # half precision only supported on CUDA\n",
        "\n",
        "        # Load model\n",
        "        model = attempt_load(weights, map_location=device)  # load FP32 model\n",
        "        imgsz = check_img_size(imgsz, s=model.stride.max())  # check img_size\n",
        "        \n",
        "        #introducing grid size\n",
        "        gs = model.stride.max()\n",
        "        #end\n",
        "\n",
        "        if half:\n",
        "            model.half()  # to FP16\n",
        "\n",
        "        # Second-stage classifier\n",
        "        classify = False\n",
        "        if classify:\n",
        "            modelc = load_classifier(name='resnet101', n=2)  # initialize\n",
        "            modelc.load_state_dict(torch.load('weights/resnet101.pt', map_location=device)['model']).to(device).eval()\n",
        "\n",
        "        # Set Dataloader\n",
        "        vid_path, vid_writer = None, None\n",
        "        \n",
        "        dataset = LoadImages(source, img_size=imgsz)\n",
        "\n",
        "        # Get names and colors\n",
        "        names = model.module.names if hasattr(model, 'module') else model.names\n",
        "        colors = [[random.randint(0, 255) for _ in range(3)] for _ in names]\n",
        "\n",
        "        colors = [[217, 175, 78]]\n",
        "\n",
        "        # Run inference\n",
        "        t0 = time.time()\n",
        "        img = torch.zeros((1, 3, imgsz, imgsz), device=device)  # init img\n",
        "        _ = model(img.half() if half else img) if device.type != 'cpu' else None  # run once\n",
        "        for path, img, im0s, vid_cap in dataset:\n",
        "            img = torch.from_numpy(img).to(device)\n",
        "            img = img.half() if half else img.float()  # uint8 to fp16/32\n",
        "            img /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
        "            if img.ndimension() == 3:\n",
        "                img = img.unsqueeze(0)\n",
        "\n",
        "            # Inference\n",
        "            t1 = time_synchronized()\n",
        "            pred = model(img, augment=True)[0]\n",
        "\n",
        "            # Apply NMS\n",
        "            final_pred = non_max_suppression(pred, 0.15, 0.3, classes=0, agnostic=True)\n",
        "            pred = non_max_suppression(pred, 0.00005, 1, classes=0, agnostic=True)\n",
        "            t2 = time_synchronized()\n",
        "\n",
        "            # Apply Classifier\n",
        "            if classify:\n",
        "                pred = apply_classifier(pred, modelc, img, im0s)\n",
        "\n",
        "            # Process detections\n",
        "            for i, det in enumerate(pred):  # detections per image\n",
        "                \n",
        "                p, s, im0 = Path(path), '', im0s.copy()\n",
        "\n",
        "                imoriginal = im0.copy()\n",
        "                #plot grid\n",
        "                numofsquares = int(imgsz/int(gs))\n",
        "                rowstep = int(im0.shape[0]/numofsquares)\n",
        "                colstep = int(im0.shape[1]/numofsquares)\n",
        "                plot_borders(im0, line_color=(0,0,0), thickness=2)\n",
        "                gridim_solo = im0.copy()\n",
        "                plot_grid(gridim_solo, pxstep=rowstep, pystep=colstep, line_color=(0,0,0), thickness=2)\n",
        "                #end plot grid\n",
        "     \n",
        "                save_path = str(finalSaveDir + \"/\" + p.name)\n",
        "                s += '%gx%g ' % img.shape[2:]  # print string\n",
        "                gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]  # normalization gain whwh\n",
        "                if len(det):\n",
        "                    # Rescale boxes from img_size to im0 size\n",
        "                    det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0.shape).round()\n",
        "\n",
        "                    # Print results\n",
        "                    for c in det[:, -1].unique():\n",
        "                        n = (det[:, -1] == c).sum()  # detections per class\n",
        "                        s += '%g %ss, ' % (n, names[int(c)])  # add to string\n",
        "                    \n",
        "                    # Write results\n",
        "                    for *xyxy, conf, cls in reversed(det):\n",
        "                        if save_img or view_img:  # Add bbox to image\n",
        "                            label = ''#'%s %.2f' % (names[int(cls)], conf)\n",
        "                            plot_one_box(xyxy, im0, label=label, color=colors[int(cls)], line_thickness=1)\n",
        "                            \n",
        "                \n",
        "\n",
        "                # Print time (inference + NMS)\n",
        "                print('%sDone. (%.3fs)' % (s, t2 - t1))\n",
        "\n",
        "                # Save results (image with detections)\n",
        "                if save_img:\n",
        "                    cv2.imwrite(save_path + \"_original.jpg\", imoriginal)\n",
        "                    cv2.imwrite(save_path, im0)\n",
        "                    cv2.imwrite(save_path + \"_grid.jpg\", gridim_solo)\n",
        "                        \n",
        "\n",
        "\n",
        "            # SAVE FINAL CROPPED IMAGES\n",
        "            # Process detections\n",
        "            for i, det in enumerate(final_pred):  # detections per image\n",
        "                \n",
        "                p, s, im0 = Path(path), '', im0s\n",
        "                im2 = im0.copy() #to use with grid/map\n",
        "                #background\n",
        "                numofsquares = int(imgsz/int(gs))\n",
        "                rowstep = int(im0.shape[0]/numofsquares)\n",
        "                colstep = int(im0.shape[1]/numofsquares)\n",
        "                plot_overlay([0,0, im2.shape[1], im2.shape[0]], im2, color=(255, 255, 255), alpha=0.7, pxstep=rowstep, pystep=colstep)\n",
        "                \n",
        "                #borders\n",
        "                plot_borders(im2, line_color=(0,0,0), thickness=2)\n",
        "                plot_borders(im0, line_color=(0,0,0), thickness=2)\n",
        "\n",
        "                save_path = str(finalSaveDir + \"/\" +  p.name)\n",
        "                s += '%gx%g ' % img.shape[2:]  # print string\n",
        "                gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]  # normalization gain whwh\n",
        "                if len(det):\n",
        "                    # Rescale boxes from img_size to im0 size\n",
        "                    det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0.shape).round()\n",
        "\n",
        "                    # Print results\n",
        "                    for c in det[:, -1].unique():\n",
        "                        n = (det[:, -1] == c).sum()  # detections per class\n",
        "                        s += '%g %ss, ' % (n, names[int(c)])  # add to string\n",
        "\n",
        "                    #FUNCTION custom crop\n",
        "                    CROP = True\n",
        "                    if CROP:\n",
        "                        fidx = 0\n",
        "                        for *xyxy, conf, cls in reversed(det):\n",
        "                            if save_img or view_img:\n",
        "                                fidx = fidx + 1\n",
        "                                crop_object(im0, xyxy, str(finalSaveDir + \"/\" +  (p.stem + \"_cropped_\" + str(fidx) + p.suffix)))\n",
        "                    #END\n",
        "                    \n",
        "                    # Write results\n",
        "                    for *xyxy, conf, cls in reversed(det):\n",
        "                        if save_img or view_img:  # Add bbox to image\n",
        "                            label = ''#'%s %.2f' % (names[int(cls)], conf)\n",
        "                            plot_one_box(xyxy, im0, label=label, color=colors[int(cls)], line_thickness=2)\n",
        "                            plot_overlay(xyxy, im2, color=colors[int(cls)], alpha=0.7, pxstep=rowstep, pystep=colstep)\n",
        "                else:\n",
        "                    cv2.imwrite(save_path + \"_not_cropped.jpg\", im0)\n",
        "\n",
        "\n",
        "                gridim = im2.copy()\n",
        "                plot_grid(gridim, pxstep=rowstep, pystep=colstep, line_color=(0,0,0), thickness=2)\n",
        "                \n",
        "                # Print time (inference + NMS)\n",
        "                print('%sDone. (%.3fs)' % (s, t2 - t1))\n",
        "\n",
        "                # Save results (image with detections)\n",
        "                if save_img:\n",
        "                    cv2.imwrite(save_path + \"_map.jpg\", gridim)\n",
        "                    cv2.imwrite(save_path + \"_final.jpg\", im0)\n",
        "\n",
        "        if save_txt or save_img:\n",
        "            s = f\"\\n{len(list(finalSaveDir.glob('labels/*.txt')))} labels saved to {finalSaveDir + '/' + 'labels'}\" if save_txt else ''\n",
        "            print(f\"Results saved to {finalSaveDir}{s}\")\n",
        "\n",
        "        print('Done. (%.3fs)' % (time.time() - t0))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiloTw_Qzx8A"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzdtwV3kpHGB"
      },
      "source": [
        "detect(model_path='/content/models/weights/RSGAI_YOLOv3.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMiHsYg5zdY9"
      },
      "source": [
        "Now that all images are cropped, we can take a look at a preview"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Zcwjc_Azan4"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "imgPath = '/content/yolov3/runs/detect/exp/train/Tomato___Leaf_Mold/image (1).JPG'\n",
        "oringinalImg = mpimg.imread(imgPath + \"_original.jpg\")\n",
        "boundingBoxesImg = mpimg.imread(imgPath)\n",
        "croppedImg = mpimg.imread(imgPath.replace(\".JPG\", \"_cropped_1.JPG\"))\n",
        "gridImg = mpimg.imread(imgPath+ \"_grid.jpg\")\n",
        "mapImg = mpimg.imread(imgPath+ \"_map.jpg\")\n",
        "finaldetectImg = mpimg.imread(imgPath+ \"_final.jpg\")\n",
        "\n",
        "print(\"Original Image:\")\n",
        "plt.axis('off')\n",
        "plt.imshow(oringinalImg)\n",
        "plt.show()\n",
        "\n",
        "print(\"Grid:\")\n",
        "plt.axis('off')\n",
        "plt.imshow(gridImg)\n",
        "plt.show()\n",
        "\n",
        "print(\"Bounding Boxes:\")\n",
        "plt.axis('off')\n",
        "plt.imshow(boundingBoxesImg)\n",
        "plt.show()\n",
        "\n",
        "print(\"Probability Map:\")\n",
        "plt.axis('off')\n",
        "plt.imshow(mapImg)\n",
        "plt.show()\n",
        "\n",
        "print(\"Final Detection:\")\n",
        "plt.axis('off')\n",
        "plt.imshow(finaldetectImg)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "print(\"Cropped Image:\")\n",
        "plt.axis('off')\n",
        "plt.imshow(croppedImg)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}